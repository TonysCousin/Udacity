{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist Sample Problem\n",
    "This is the optional problem provided in module 3 of the Udacity Deep Learning nanodegree program.  I am building it on my own, after completing the course.\n",
    "\n",
    "I'm trying to decide on a process flow.  Here I'll use the following phases:\n",
    "1. Project setup & acquire data\n",
    "2. Load & prepare the data (includes spot checking & sanity checking for cleanliness & appropriate content)\n",
    "3. Build the model\n",
    "4. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 1:** \n",
    "Data, provided by Udacity, already exists elsewhere in my local directory.  So this is probably a lot shorter than it would normally be.  This data set comes from [2017 ISIC Challenge on Skin Lesion Analysis Towards Melanoma Detection] (https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a).\n",
    "\n",
    "**Phase 2:**\n",
    "* These images are large, and vary in size.  Since I'm looking for lots of details to help improve classification, I don't want to throw away lots of info by unnecessarily croping or downsampling the images. Therefore, I first need to go through the data set and figure out the min/max sizes.  From there I can figure out what is possible.\n",
    "* Consisder using scaling like what is found in https://opensource.com/life/15/2/resize-images-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "import torch\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# returns the number of GiB of cuda memory used\n",
    "def memory_gb():\n",
    "    max_val = 0\n",
    "    if (train_on_gpu):\n",
    "        gb_alloc = torch.cuda.memory_allocated() / 1024 / 1024 / 1024\n",
    "        gb_res   = torch.cuda.memory_reserved() /  1024 / 1024 / 1024\n",
    "        max_val = gb_alloc\n",
    "        if gb_res > gb_alloc:\n",
    "            max_val = gb_res\n",
    "    return max_val\n",
    "\n",
    "# compares current memory use to a previous value & returns the larger\n",
    "def memory_max(prev_max):\n",
    "    mem = memory_gb()\n",
    "    if (mem > prev_max):\n",
    "        return mem\n",
    "    else:\n",
    "        return prev_max\n",
    "\n",
    "# printable report of cuda memory used\n",
    "def memory_rpt(label=\"\"):\n",
    "    print(\"      {}: mem {:.3f} GB\".format(label, memory_gb()))\n",
    "    \n",
    "# garbage collection\n",
    "def memory_clean():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT_GOAL = 768 #pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n",
      "Using image size:  768\n",
      "Num training images:    2000\n",
      "Num validation images:  150\n",
      "Num test images:        600\n",
      "training data =  Dataset ImageFolder\n",
      "    Number of datapoints: 2000\n",
      "    Root location: /home/ubuntu/ml/dermatologist/data/train/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(768, 768), scale=(0.4, 1.1), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
      "               ColorJitter(brightness=[0.8, 1.2], contrast=[0.8, 1.2], saturation=[0.8, 1.2], hue=[-0.2, 0.2])\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               RandomVerticalFlip(p=0.5)\n",
      "               RandomAffine(degrees=(-30, 30), translate=(0.2, 0.2))\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "\n",
    "# define training and test data directories\n",
    "#data_dir = '/home/starkj/Udacity/repo/dermatologist/data/' #for local VM\n",
    "data_dir = '/home/ubuntu/ml/dermatologist/data/' #for AWS server\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "val_dir   = os.path.join(data_dir, \"valid/\")\n",
    "test_dir  = os.path.join(data_dir, 'test/')\n",
    "\n",
    "# classes are folders in each directory with these names\n",
    "classes = ['melanoma', 'nevus', 'seborrheic_keratosis']\n",
    "\n",
    "# load and transform data using ImageFolder\n",
    "\n",
    "# load and transform data using ImageFolder\n",
    "### NOTE:  very important!  Some transforms work on PIL images and some on tensors.  Apply all the\n",
    "###        PIL transforms first, then convert the result to a Tensor, then we can apply further\n",
    "###        transforms if desired.\n",
    "\n",
    "image_size = HEIGHT_GOAL\n",
    "print(\"Using image size: \", image_size)\n",
    "\n",
    "full1 = transforms.Compose([transforms.RandomResizedCrop(image_size, scale=(0.4, 1.1)), \n",
    "                                     transforms.ColorJitter(brightness=0.2, contrast=0.2,\n",
    "                                                            saturation=0.2, hue=0.2),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.RandomVerticalFlip(),\n",
    "                                     transforms.RandomAffine(degrees=30, translate=(0.2, 0.2)),\n",
    "                                     transforms.ToTensor()])\n",
    "no_xform = transforms.ToTensor()\n",
    "crop_only = transforms.Compose([transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "#####\n",
    "##### select the transform set here!\n",
    "#####\n",
    "data_transform = full1\n",
    "\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
    "val_data   = datasets.ImageFolder(val_dir, transform=data_transform)\n",
    "test_data  = datasets.ImageFolder(test_dir, transform=data_transform)\n",
    "\n",
    "# print out some data stats\n",
    "print('Num training images:   ', len(train_data))\n",
    "print('Num validation images: ', len(val_data))\n",
    "print('Num test images:       ', len(test_data))\n",
    "print('training data = ', train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration support function\n",
    "class ImageDatasetStats:\n",
    "    def __init__(self):\n",
    "        self.min_w  = 99999\n",
    "        self.min_w2 = 99999 #second smallest\n",
    "        self.max_w  = 0\n",
    "        self.max_w2 = 0 #second largest\n",
    "        self.min_h  = 99999\n",
    "        self.min_h2 = 99999\n",
    "        self.max_h  = 0\n",
    "        self.max_h2 = 0\n",
    "\n",
    "    # accessors\n",
    "    def get_min_w(self):\n",
    "        return (self.min_w, self.min_w2)\n",
    "\n",
    "    def get_max_w(self):\n",
    "        return (self.max_w, self.max_w2)\n",
    "\n",
    "    def get_min_h(self):\n",
    "        return (self.min_h, self.min_h2)\n",
    "\n",
    "    def get_max_h(self):\n",
    "        return (self.max_h, self.max_h2)\n",
    "    \n",
    "    # set the first and/or second smallest min dimension if new item exceeds\n",
    "    def adjust_mins(self, min1, min2, val):\n",
    "        if val < min1:\n",
    "            min2 = min1\n",
    "            min1 = val\n",
    "        elif val < min2:\n",
    "            min2 = val\n",
    "        return (min1, min2)\n",
    "\n",
    "    # set the first and/or second largest max dimension if new item exceeds\n",
    "    def adjust_maxs(self, max1, max2, val):\n",
    "        if val > max1:\n",
    "            max2 = max1\n",
    "            max1 = val\n",
    "        elif val > max2:\n",
    "            max2 = val\n",
    "        return (max1, max2)\n",
    "\n",
    "    # iterate through an image dataset, finding the first and second largest and smallest values in each dimension\n",
    "    # (width and height)\n",
    "    #    data: ImageFolder\n",
    "    def find2dim_extremes(self, data):\n",
    "        for i in range(len(data)):\n",
    "            im = train_data.__getitem__(i)[0]\n",
    "            h = im.shape[1]\n",
    "            w = im.shape[2]\n",
    "            mins = self.adjust_mins(self.min_w, self.min_w2, w)\n",
    "            maxs = self.adjust_maxs(self.max_w, self.max_w2, w)\n",
    "            self.min_w  = mins[0]\n",
    "            self.min_w2 = mins[1]\n",
    "            self.max_w  = maxs[0]\n",
    "            self.max_w2 = maxs[1]\n",
    "            \n",
    "            mins = self.adjust_mins(self.min_h, self.min_h2, h)\n",
    "            maxs = self.adjust_maxs(self.max_h, self.max_h2, h)\n",
    "            self.min_h  = mins[0]\n",
    "            self.min_h2 = mins[1]\n",
    "            self.max_h  = maxs[0]\n",
    "            self.max_h2 = maxs[1]\n",
    "\n",
    "            if i%10 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "            #print(\"\\nimage shape = \", im.shape)\n",
    "            #print(\"w = \", w, \", h = \", h)\n",
    "            #print(\"width mins & maxes = \", self.min_w, self.min_w2, self.max_w, self.max_w2)\n",
    "            #print(\"height mins & maxs = \", self.min_h, self.min_h2, self.max_h, self.max_h2)\n",
    "   \n",
    "    # iterate through a dataset and count how many images are in each size bin, both height & width\n",
    "    # data: a DataSet of images\n",
    "    def generate_histogram(self, minw, maxw, minh, maxh, num_bins, data):\n",
    "        w_bin_width = int((maxw - minw)/num_bins)\n",
    "        h_bin_width = int((maxh - minh)/num_bins)\n",
    "        \n",
    "        # initialize the arrays that will count the number of items in each size bin\n",
    "        w_count = []\n",
    "        h_count = []\n",
    "        for i in range(num_bins):\n",
    "            w_count.append(0)\n",
    "            h_count.append(0)\n",
    "        \n",
    "        # determine the boundaries of each width bin\n",
    "        w_bin_upper_bound = []\n",
    "        for i in range(num_bins):\n",
    "            w_bin_upper_bound.append(minw + (i+1)*w_bin_width - 1)\n",
    "        w_bin_upper_bound[num_bins-1] = maxw #adjust size of final bin to account for bin width rounding\n",
    "        #print(\"w_bin_upper_bound = \", w_bin_upper_bound)\n",
    "        \n",
    "        # determine the boudnaries of each height bin\n",
    "        h_bin_upper_bound = []\n",
    "        for i in range(num_bins):\n",
    "            h_bin_upper_bound.append(minh + (i+1)*h_bin_width - 1)\n",
    "        h_bin_upper_bound[num_bins-1] = maxh #adjust size of final bin to account for bin width rounding\n",
    "        #print(\"h_bin_upper_bound = \", h_bin_upper_bound)\n",
    "        \n",
    "        # count the number of images that fall into each bin\n",
    "        dsize = len(data)\n",
    "        print(\"Binning images in dataset size \", dsize)\n",
    "        for i in range(dsize):\n",
    "            im = data.__getitem__(i)[0]\n",
    "            h = im.shape[1]\n",
    "            w = im.shape[2]\n",
    "            #print(\"Image \", i, \": h = \", h, \", w = \", w)\n",
    "\n",
    "            for j in range(num_bins):\n",
    "                if w <= w_bin_upper_bound[j]:\n",
    "                    w_count[j] += 1\n",
    "                    break\n",
    "            for j in range(num_bins):\n",
    "                if h <= h_bin_upper_bound[j]:\n",
    "                    h_count[j] += 1\n",
    "                    break\n",
    "        return w_count, h_count, w_bin_upper_bound, h_bin_upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nprint(\"train_data shape = \", type(train_data))\\n\\n# Make a loop to look at each image and determine range of sizes\\nstats = ImageDatasetStats()\\nstats.find2dim_extremes(train_data)\\nminw = stats.get_min_w()\\nmaxw = stats.get_max_w()\\nminh = stats.get_min_h()\\nmaxh = stats.get_max_h()\\nprint(\" \")\\nprint(\"Width extremes  = \", minw, maxw)\\nprint(\"Height extremes = \", minh, maxh)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### data exploration only\n",
    "'''\n",
    "\n",
    "print(\"train_data shape = \", type(train_data))\n",
    "\n",
    "# Make a loop to look at each image and determine range of sizes\n",
    "stats = ImageDatasetStats()\n",
    "stats.find2dim_extremes(train_data)\n",
    "minw = stats.get_min_w()\n",
    "maxw = stats.get_max_w()\n",
    "minh = stats.get_min_h()\n",
    "maxh = stats.get_max_h()\n",
    "print(\" \")\n",
    "print(\"Width extremes  = \", minw, maxw)\n",
    "print(\"Height extremes = \", minh, maxh)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling variable image sizes\n",
    "\n",
    "Clearly, from above, we have a wide range of image sizes, by almost 10x.  Cropping is not a good answer to this, because many of the images are framed tightly around the artifact of interest, so even a little cropping will be throwing away valuable information.  Other alternatives, which might be combined:\n",
    "\n",
    "1) Downsample the larger images - probably useful for a certain range of downsampling (e.g. up to 4x?) but we stand to lose too much info in the largest images if they are downsampled a lot.  See https://scikit-image.org/docs/dev/auto_examples/transform/plot_rescale.html\n",
    "\n",
    "2) Throw out the smallest and/or largest images as outliers - if there are only a few way out on the tails of the distribution, this could help reduce the magnitude of the problem, but the statistics above only show that there are at least two images at each of the very extremes.\n",
    "\n",
    "3) Build a network that handles variable image sizes - there are some techniques for this, but it involves advanced CNN construction, and adds difficulty to the final FC classification layer.  See:\n",
    "    * https://stats.stackexchange.com/questions/388859/is-it-possible-to-give-variable-sized-images-as-input-to-a-convolutional-neural\n",
    "    * https://www.reddit.com/r/MachineLearning/comments/akbe39/d_best_approach_to_variable_image_sizes_for_image/\n",
    "    * Thesis paper in Downloads folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstats = ImageDatasetStats()\\nres = stats.generate_histogram(576, 6748, 540, 4499, 11, train_data)\\n\\nprint(\"width counts = \", res[0])\\nprint(\"width Ubound = \", res[2])\\nprint(\"height counts =\", res[1])\\nprint(\"height Ubound =\", res[3])\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### data exploration only\n",
    "'''\n",
    "stats = ImageDatasetStats()\n",
    "res = stats.generate_histogram(576, 6748, 540, 4499, 11, train_data)\n",
    "\n",
    "print(\"width counts = \", res[0])\n",
    "print(\"width Ubound = \", res[2])\n",
    "print(\"height counts =\", res[1])\n",
    "print(\"height Ubound =\", res[3])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on variable image sizes\n",
    "As the cell above shows, the distribution is far from Gaussian, with ~1/3 of all the images in the lowest size bin.  The upper four size bins combined are about 7% of the images in the training set.  Therefore, for the sake of what is supposed to be a fairly simple exercise here, I'll ignore images with w > 4502 or h > 3052 pixels.  For the remainder, I'll downsample the larger ones and pad the smaller ones.  Since most of the images are in the smallest bin, I don't want to perturb them too much.  The smallest being h=540 and w=576, it probably isn't prudent to pad more than 30%, so set my input image size to 700x748 pixels.  Anything smaller will be padded; anything larger will be downsampled and/or cropped.\n",
    "\n",
    "Problem is, the standard DataLoader won't accept input images that have variable sizes, per below, so need to figure out how to get around that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# This code fragment pulled from Udacity DLND module 3 homework.\\n# Note that it only works if all images are identical size (dataiter assumes this)\\n\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\\n\\n# define dataloader parameters\\nbatch_size = 32\\nnum_workers=0\\n\\n# prepare data loaders\\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \\n                                           num_workers=num_workers, shuffle=True)\\nval_loader   = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\\n                                           num_workers=num_workers, shuffle=True)\\ntest_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \\n                                           num_workers=num_workers, shuffle=True)\\n\\n# obtain one batch of training images\\ndataiter = iter(train_loader) ######### THIS LINE BREAKS - can\\'t handle variable image sizes\\nimages, labels = dataiter.next()\\nimages = images.numpy() # convert images to numpy for display\\nprint(\"images shape = \", images.shape)\\n\\n# plot the images in the batch, along with the corresponding labels\\nfig = plt.figure(figsize=(25, 4))\\nfor idx in np.arange(20):\\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\\n    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\\n    ax.set_title(classes[labels[idx]])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### data exploration only - can't use this as is\n",
    "'''\n",
    "\n",
    "# This code fragment pulled from Udacity DLND module 3 homework.\n",
    "# Note that it only works if all images are identical size (dataiter assumes this)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# define dataloader parameters\n",
    "batch_size = 32\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader) ######### THIS LINE BREAKS - can't handle variable image sizes\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "print(\"images shape = \", images.shape)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading image data of variable sizes\n",
    "\n",
    "* There is an interesting answer for how to do this at https://discuss.pytorch.org/t/torchvision-and-dataloader-different-images-shapes/41026/3. Code from this page is copied below for ease of reading (without scroll bars).\n",
    "* Another, maybe more useful answer, with links to lots of other answers: https://stackoverflow.com/questions/55041080/how-does-pytorch-dataloader-handle-variable-size-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### for image exploration only\n",
    "### I don't think I want to use this, as it has masks that appear to be a substitute for resizing the images,\n",
    "### but it looks like it requires each image to be in a fixed size tensor, which I don't want to do.\n",
    "\n",
    "def default_collate(batch):\n",
    "    \"\"\"\n",
    "    Override `default_collate` https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader\n",
    "\n",
    "    Reference:\n",
    "    def default_collate(batch) at https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader\n",
    "    https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278/3\n",
    "    https://github.com/pytorch/pytorch/issues/1512\n",
    "\n",
    "    We need our own collate function that wraps things up (imge, mask, label).\n",
    "\n",
    "    In this setup,  batch is a list of tuples (the result of calling: img, mask, label = Dataset[i].\n",
    "    The output of this function is four elements:\n",
    "        . data: a pytorch tensor of size (batch_size, c, h, w) of float32 . Each sample is a tensor of shape (c, h_,\n",
    "        w_) that represents a cropped patch from an image (or the entire image) where: c is the depth of the patches (\n",
    "        since they are RGB, so c=3),  h is the height of the patch, and w_ is the its width.\n",
    "        . mask: a list of pytorch tensors of size (batch_size, 1, h, w) full of 1 and 0. The mask of the ENTIRE image (no\n",
    "        cropping is performed). Images does not have the same size, and the same thing goes for the masks. Therefore,\n",
    "        we can't put the masks in one tensor.\n",
    "        . target: a vector (pytorch tensor) of length batch_size of type torch.LongTensor containing the image-level\n",
    "        labels.\n",
    "    :param batch: list of tuples (img, mask, label)\n",
    "    :return: 3 elements: tensor data, list of tensors of masks, tensor of labels.\n",
    "    \"\"\"\n",
    "    data = torch.stack([item[0] for item in batch])\n",
    "    mask = [item[1] for item in batch]  # each element is of size (1, h*, w*). where (h*, w*) changes from mask to another.\n",
    "    target = torch.LongTensor([item[2] for item in batch])  # image labels.\n",
    "\n",
    "    return data, mask, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Defining a custom collator for the DataLoader since its default assumes all raw images are identical\n",
    "# size.  In our case they are not.  This function not only loads different size images, but manipulates\n",
    "# them to be identical size (downsampling and cropping), because the CNN being used in this project\n",
    "# is assuming that all images are identical size.\n",
    "# This function is modified from https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/\n",
    "#\n",
    "# `batch` is a list of tuple where first element is image tensor and\n",
    "# second element is corresponding label\n",
    "#\n",
    "# return is a tuple of (data, target), where data is a tensor of shape [b, c, h, w] and\n",
    "# target is a list of labels\n",
    "\n",
    "AR_GOAL = 1.333333 # represents 768x1024\n",
    "WIDTH_GOAL = HEIGHT_GOAL * AR_GOAL\n",
    "\n",
    "def variable_collate(batch):\n",
    "    data_list = []\n",
    "    for item in batch:\n",
    "        \n",
    "        # get info on the raw image shape\n",
    "        raw_image = item[0]\n",
    "        h = raw_image.shape[1]\n",
    "        w = raw_image.shape[2]\n",
    "        ar = w/h\n",
    "        \n",
    "        # if the aspect ratio is larger than desired, then we scale by the height, otherwise by width\n",
    "        if ar > AR_GOAL:\n",
    "            scale_factor = HEIGHT_GOAL / h\n",
    "        else:\n",
    "            scale_factor = WIDTH_GOAL / w\n",
    "        #print(\"In variable_collate: raw h = \", h, \", w = \", w, \", scale_factor = \", scale_factor)\n",
    "        #print(\"  raw_image = \", raw_image.shape)\n",
    "            \n",
    "        # convert the raw_image tensor into a PIL Image object\n",
    "        image = transforms.ToPILImage()(raw_image.squeeze_(0))\n",
    "        #print(\"  After transform, image = \", image.size)\n",
    "        \n",
    "        # scale it, preserving aspect ratio\n",
    "        resized = image.resize((int(w*scale_factor), int(h*scale_factor)))\n",
    "        width = resized.width\n",
    "        height = resized.height\n",
    "        #print(\"  resized w = \", width, \", h = \", height)\n",
    "            \n",
    "        # now we have an image that doesn't necessarily match our goal aspect ratio, but is at least\n",
    "        # as large as our goal dimensions in both directions; need to center-crop it - \n",
    "        # DON'T try to make it square!\n",
    "        if width > WIDTH_GOAL:\n",
    "            left = (width - WIDTH_GOAL) // 2\n",
    "        else:\n",
    "            left = 0\n",
    "        right = left + WIDTH_GOAL #don't subtract 1; this is the next pixel beyond the edge of the image\n",
    "        \n",
    "        if height > HEIGHT_GOAL:\n",
    "            upper = (height - HEIGHT_GOAL) // 2\n",
    "        else:\n",
    "            upper = 0\n",
    "        lower = upper + HEIGHT_GOAL #don't subtract 1\n",
    "        #print(\"  cropping to left = \", left, \", right = \", right, \", upper = \", upper, \", lower = \", lower)\n",
    "        \n",
    "        cropped_image = resized.crop((left, upper, right, lower))\n",
    "        \n",
    "        # convert the properly sized image back to a tensor\n",
    "        cropped_tensor = transforms.ToTensor()(cropped_image).unsqueeze_(0)\n",
    "        data_list.append(cropped_tensor)\n",
    "        #print(\"  cropped_tensor = \", cropped_tensor.shape)\n",
    "        \n",
    "\n",
    "    # assemble all of the images into the data tensor for output\n",
    "    data = torch.stack(data_list).squeeze()\n",
    "    #print(\"  returning data = \", data.shape)\n",
    "    \n",
    "    # gather the target labels for each image\n",
    "    target = [item[1] for item in batch]\n",
    "    target = torch.LongTensor(target)\n",
    "    \n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader parameters\n",
    "batch_size = 20\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, collate_fn=variable_collate, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_data, collate_fn=variable_collate, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, collate_fn=variable_collate, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# This code fragment pulled from Udacity DLND module 3 homework.\\n# Note that it only works if all images are identical size (dataiter assumes this)\\n\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\\n\\n# obtain one batch of training images - these need to be in a tensor of shape [b, c, w, h]\\ndataiter = iter(train_loader)\\nimages, labels = dataiter.next()\\nprint(\"images type from dataiter = \", type(images))\\nimages = images.numpy() # convert images to numpy for display\\nprint(\"images shape = \", images.shape)\\n\\n# plot the images in the batch, along with the corresponding labels\\nfig = plt.figure(figsize=(25, 8)) #numbers indicate horiz & vert spacing between images\\nfor idx in np.arange(32):\\n    ax = fig.add_subplot(4, 32/4, idx+1, xticks=[], yticks=[]) #num rows, num images/row\\n    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\\n    ax.set_title(classes[labels[idx]])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### For image exploration only\n",
    "'''\n",
    "# This code fragment pulled from Udacity DLND module 3 homework.\n",
    "# Note that it only works if all images are identical size (dataiter assumes this)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# obtain one batch of training images - these need to be in a tensor of shape [b, c, w, h]\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"images type from dataiter = \", type(images))\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "print(\"images shape = \", images.shape)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 8)) #numbers indicate horiz & vert spacing between images\n",
    "for idx in np.arange(32):\n",
    "    ax = fig.add_subplot(4, 32/4, idx+1, xticks=[], yticks=[]) #num rows, num images/row\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images look pretty good!  Sizes of the objects of interest aren't as uniform as I'd like, but for the purposes of this exercise, I think close enough.  To get better, it's probably worth doing some pre-processing with some sort of edge/feature detection to define a bounding box in the raw image, then crop around that before doing the scaling I've done here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model\n",
    "\n",
    "I would like to use transfer learning with one of the big CNN's, but I need one that will use my chosen image size, or something close to it.\n",
    "* VGG uses 224x224\n",
    "* ResNet50 - not clear, but it looks like it can handle different sizes, up to 640 on short side.\n",
    "* GoogLeNet 50 appears to take 224x224 also.\n",
    "\n",
    "I'm starting to think all of these models are built for 224 square as a de facto standard for benchmarking.  Therefore, I choose to write my own model from scratch.  There is no doubt it will perform much worse than these world class models, but it will have one advantage in being able to use more input data (more pixels in the input images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkinCancerCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkinCancerCnn, self).__init__()\n",
    "        \n",
    "        self.CONV_OUT_FEATURES = int(128 * (768/2/2/2/2/2) * (1024/2/2/2/2/2))\n",
    "        print(\"CONV_OUT_FEATURES = \", self.CONV_OUT_FEATURES)\n",
    "        \n",
    "        # Applying batch norm to all but first and last layers, per guidance from the DL nanodegree\n",
    "        # class, module 5.  Not clear why this restriction, however.\n",
    "        \n",
    "        #input images are 768x1024 pixels, full coloer (3 channels)\n",
    "        self.c1 = nn.Conv2d(3, 8, 2, padding=1) #stride = 1\n",
    "\n",
    "        self.c2 = nn.Conv2d(8, 16, 2, padding=1, bias=False)\n",
    "        self.b2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.c3 = nn.Conv2d(16, 32, 2, padding=1, bias=False)\n",
    "        self.b3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(32, 64, 2, padding=1, bias=False) #must not use bias if using batchnorm!\n",
    "        self.b4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.c5 = nn.Conv2d(64, 128, 2, padding=1, bias=False)\n",
    "        self.b5 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        #linear classifier\n",
    "        self.fc1 = nn.Linear(self.CONV_OUT_FEATURES, 3082)\n",
    "        self.fc2 = nn.Linear(3082, 96)\n",
    "        self.fc3 = nn.Linear(96, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #memory_rpt(\"forward top\")\n",
    "        #print(\"      at entry, x.shape = \", x.shape)\n",
    "        \n",
    "        x = F.relu(self.c1(x)) #output 8 x 768 x 1024\n",
    "        x = self.pool(x)       #output 8 x 384 x 512\n",
    "        \n",
    "        # Experimenting with placing the BN function before or after the relu in each layer.\n",
    "        # It seems there is some debate about which is best.\n",
    "        x = self.c2(x)         #ouptut 16 x 384 x 512\n",
    "        x = F.relu(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.pool(x)       #output 16 x 192 x 256\n",
    "        \n",
    "        x = self.c3(x)         #output 32 x 192 x 256\n",
    "        x = F.relu(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.pool(x)       #output 32 x 96 x 128\n",
    "        \n",
    "        x = self.c4(x)         #output 64 x 96 x 128\n",
    "        x = F.relu(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.pool(x)       #output 64 x 48 x 64\n",
    "        \n",
    "        x = self.c5(x)         #output 128 x 48 x 64\n",
    "        x = F.relu(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.pool(x)       #output 128 x 24 x 32\n",
    "        \n",
    "        ### CONSIDER putting garbage collectiion in between these layers - worth some experimenting.\n",
    "        #memory_rpt(\"  fwd conv \")\n",
    "        \n",
    "        conv_out = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        #print(\"      after conv, x.shape = \", x.shape, \"conv_out = \", conv_out)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1) #flatten the convolved image\n",
    "        #memory_rpt(\"  fwd view \")\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x)) # softmax will be applied during training (as part of loss function), not here\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "# Trains the model\n",
    "# run_id:          identifier used in checkpoint file naming\n",
    "# prev_run:        identifier used in file naming of previous checkpoint to be read at beginning\n",
    "# starting_point:  input ID of the checkpoint file to use as a starting point (0 = no checkpoint)\n",
    "# num_epochs:      number of epochs to run in this call\n",
    "# save_interval:   save a checkpoint every this many epochs\n",
    "\n",
    "def train_it(run_id=\"XXX\", prev_run=\"XXX\", starting_point=0, num_epochs=10, save_interval=10, learn_rate=0.001):\n",
    "    \n",
    "    min_val_loss = 999999.0\n",
    "    min_val_epoch = 0\n",
    "    val_retry_limit = 6\n",
    "    file_prefix = \"model/derma\"\n",
    "\n",
    "    if starting_point == 0:\n",
    "        print(\"Creating a new, untrained model.\")\n",
    "        model = SkinCancerCnn()\n",
    "    else:\n",
    "        checkpoint_file = \"{}{}-{:03d}.pt\".format(file_prefix, prev_run, starting_point)\n",
    "        print(\"Retrieving partially trained model from \", checkpoint_file)\n",
    "        model = torch.load(checkpoint_file)\n",
    "\n",
    "    print(model)\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        print(\"Training on GPU\")\n",
    "\n",
    "    LEARNING_RATE = learn_rate\n",
    "    print(\"LR = \", LEARNING_RATE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "    print(\"Entering training loop.\")\n",
    "    #memory_rpt()\n",
    "    max_mem = memory_gb()\n",
    "    start_time = time.perf_counter() #was time.clock()\n",
    "\n",
    "    for epoch in range(starting_point, starting_point+num_epochs):\n",
    "        model.train() #put it into training mode\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            if train_on_gpu:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "\n",
    "            #memory_rpt(\"  after model run\")\n",
    "            max_mem = memory_max(max_mem)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()*data.size(0) #CrossEntropy already divides by num items in batch\n",
    "\n",
    "            # ensure garbage doesn't fill up the gpu memory\n",
    "            memory_clean()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # validate the training on this epoch\n",
    "        #print(\"    // Begin validation //\")\n",
    "        model.eval() #put into evaluation mode\n",
    "        val_loss = 0.0\n",
    "        for data, target in val_loader:\n",
    "            if train_on_gpu:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            output = model(data)\n",
    "            #memory_rpt(\"  after VAL model run\")\n",
    "            max_mem = memory_max(max_mem)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "\n",
    "            # ensure garbage doesn't fill up the gpu memory\n",
    "            memory_clean()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # determine if training is complete, based on validation loss hitting a minimum recently\n",
    "        flag = ' ' \n",
    "        if val_loss <= min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_epoch = epoch\n",
    "            flag = '*'\n",
    "            \n",
    "            # save a copy of the model\n",
    "            best_model = model\n",
    "            \n",
    "        # determine epoch duration and estimate remaining time\n",
    "        current_time = time.perf_counter() #was time.clock()\n",
    "        avg_duration = (current_time - start_time) / (epoch - starting_point + 1) / 60.0 #minutes\n",
    "        remaining_time_minutes = (starting_point + num_epochs - epoch - 1) * avg_duration\n",
    "        if remaining_time_minutes > 80:\n",
    "            rem_time = remaining_time_minutes / 60.0\n",
    "            time_est_msg = \"{:4.1f} hours rem.\".format(rem_time)\n",
    "        else:\n",
    "            time_est_msg = \"{:.0f} min rem.\".format(remaining_time_minutes)\n",
    "\n",
    "\n",
    "        # show epoch results\n",
    "        max_mem = memory_max(max_mem)\n",
    "        memory_clean()\n",
    "        print(\"///// Epoch {:3d}: train loss = {:.4f}, val loss = {:.4f}{}, mem {:.2f}/{:.2f} GB. Avg {:.1f} min/epoch; {}\".format(\n",
    "            epoch, train_loss, val_loss, flag, memory_gb(), max_mem, avg_duration, time_est_msg))\n",
    "\n",
    "        # save a checkpoint if it's appropriate\n",
    "        if epoch > 0  and  epoch % save_interval == 0:\n",
    "            save_file = \"{}{}-{:03d}.pt\".format(file_prefix, run_id, epoch)\n",
    "            print(\"      Saving checkpoint \", save_file)\n",
    "            torch.save(model, save_file)\n",
    "        \n",
    "        if epoch > (min_val_epoch + val_retry_limit):\n",
    "            print(\"      Training terminated due to no further decrease in validation loss.\")\n",
    "            break\n",
    "            \n",
    "    # save a checkpoint of the best epoch\n",
    "    save_file = \"{}{}-{:03d}.pt\".format(file_prefix, run_id, min_val_epoch)\n",
    "    print(\"      Saving best checkpoint \", save_file)\n",
    "    torch.save(best_model, save_file)\n",
    "\n",
    "    print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving partially trained model from  model/derma15-008.pt\n",
      "SkinCancerCnn(\n",
      "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (b5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=98304, out_features=3082, bias=True)\n",
      "  (fc2): Linear(in_features=3082, out_features=96, bias=True)\n",
      "  (fc3): Linear(in_features=96, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Training on GPU\n",
      "LR =  2e-06\n",
      "Entering training loop.\n",
      "///// Epoch   8: train loss = 0.9338, val loss = 1.0493*, mem 3.88/8.55 GB. Avg 12.1 min/epoch;  9.9 hours rem.\n",
      "///// Epoch   9: train loss = 0.9341, val loss = 1.0607 , mem 3.88/8.55 GB. Avg 12.1 min/epoch;  9.6 hours rem.\n",
      "///// Epoch  10: train loss = 0.9277, val loss = 1.0478*, mem 3.88/8.55 GB. Avg 12.0 min/epoch;  9.4 hours rem.\n",
      "      Saving checkpoint  model/derma16-010.pt\n",
      "///// Epoch  11: train loss = 0.9217, val loss = 1.0588 , mem 3.88/8.55 GB. Avg 12.1 min/epoch;  9.3 hours rem.\n",
      "///// Epoch  12: train loss = 0.9182, val loss = 1.0536 , mem 3.88/8.55 GB. Avg 12.1 min/epoch;  9.0 hours rem.\n",
      "///// Epoch  13: train loss = 0.9201, val loss = 1.0633 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  8.8 hours rem.\n",
      "///// Epoch  14: train loss = 0.9197, val loss = 1.0471*, mem 3.88/8.55 GB. Avg 12.0 min/epoch;  8.6 hours rem.\n",
      "///// Epoch  15: train loss = 0.9081, val loss = 1.0477 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  8.4 hours rem.\n",
      "///// Epoch  16: train loss = 0.9119, val loss = 1.0503 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  8.2 hours rem.\n",
      "///// Epoch  17: train loss = 0.9114, val loss = 1.0476 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  8.0 hours rem.\n",
      "///// Epoch  18: train loss = 0.9034, val loss = 1.0444*, mem 3.88/8.55 GB. Avg 12.0 min/epoch;  7.8 hours rem.\n",
      "///// Epoch  19: train loss = 0.9037, val loss = 1.0402*, mem 3.88/8.55 GB. Avg 12.0 min/epoch;  7.6 hours rem.\n",
      "///// Epoch  20: train loss = 0.9106, val loss = 1.0536 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  7.4 hours rem.\n",
      "      Saving checkpoint  model/derma16-020.pt\n",
      "///// Epoch  21: train loss = 0.9061, val loss = 1.0873 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  7.2 hours rem.\n",
      "///// Epoch  22: train loss = 0.9041, val loss = 1.0411 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  7.0 hours rem.\n",
      "///// Epoch  23: train loss = 0.8977, val loss = 1.0407 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  6.8 hours rem.\n",
      "///// Epoch  24: train loss = 0.8997, val loss = 1.0566 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  6.6 hours rem.\n",
      "///// Epoch  25: train loss = 0.8979, val loss = 1.0296*, mem 3.88/8.55 GB. Avg 12.0 min/epoch;  6.4 hours rem.\n",
      "///// Epoch  26: train loss = 0.8994, val loss = 1.0492 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  6.2 hours rem.\n",
      "///// Epoch  27: train loss = 0.8976, val loss = 1.0505 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  6.0 hours rem.\n",
      "///// Epoch  28: train loss = 0.8948, val loss = 1.0626 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  5.8 hours rem.\n",
      "///// Epoch  29: train loss = 0.8935, val loss = 1.0604 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  5.6 hours rem.\n",
      "///// Epoch  30: train loss = 0.8918, val loss = 1.0589 , mem 3.88/8.55 GB. Avg 12.0 min/epoch;  5.4 hours rem.\n",
      "      Saving checkpoint  model/derma16-030.pt\n",
      "///// Epoch  31: train loss = 0.8913, val loss = 1.0816 , mem 3.88/8.55 GB. Avg 12.1 min/epoch;  5.2 hours rem.\n",
      "///// Epoch  32: train loss = 0.8920, val loss = 1.0623 , mem 3.88/8.55 GB. Avg 12.1 min/epoch;  5.0 hours rem.\n",
      "      Training terminated due to no further decrease in validation loss.\n",
      "      Saving best checkpoint  model/derma16-025.pt\n",
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RUN      = \"16\"  ##### set this before each run!\n",
    "prev_run = \"15\"\n",
    "\n",
    "train_it(RUN, prev_run, starting_point=8, num_epochs=50, learn_rate=0.000002)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AWS\n",
    "Ran the above code for 2 epochs on the local computer to determine that the model is functional and seems to be trainable (loss decreased).\n",
    "\n",
    "Now it's time to move it to an AWS instance in order to train.  I discovered my existing instance does not have any storage attached (I had been using a separate EBS volume, and deleted it after class).  Therefore, choosing a new instance:  <code>g2.2xlarge</code>, which has 1 GPU and 2 cores of CPU (2 threads each) and 16 GB of memory.\n",
    "\n",
    "System volume is /dev/sda1.  Working data storage is on /dev/sdb (60 GB).  Instance ID is i-0be255fe5cb1becf1.  Username is default \"ubuntu\".  Changed pwd to H1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Ubuntu VM disk size problem\n",
    "\n",
    "Diversion for a few days...\n",
    "\n",
    "Got into trouble by filling up my hard drive.  Took a long time to move data off and back it up, then figure out how to extend the drive space in VMWare.  \n",
    "* First, shut down the VM, then in Settings select Extend drive space.  This enlarges the physical drive only.  It doesn't extend the partition or the file system in the partition.\n",
    "* Next use gparted to enlarge the physical partition (I did not have a logical volume, so LVM was not an option).  Decent instructions here to get started, then just follow intuitin and the UI:  https://unix.stackexchange.com/questions/196512/how-to-extend-filesystem-partition-on-ubuntu-vm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transfer\n",
    "\n",
    "Now that VM drive space problem is solved, I re-downloaded all of the data from the original Udacity problem and moved it into my Github repo under dermatologist/data.  This took quite a while, as each push has to complete within a certain timeout period (not clear to me what causes that, but it appears it's my host screen saver time that essentially shuts down VM activity).\n",
    "\n",
    "Attempted to pull the entire dermatologist branch down to my AWS server, but ran out of drive space there. Cleaned it up and pulled all the data.  Now ready to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "### Error using GPU\n",
    "Upon first run, I got the following error:\n",
    "\n",
    "<code>\n",
    "/home/ubuntu/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:102: UserWarning: \n",
    "    Found GPU0 GRID K520 which is of cuda capability 3.0.\n",
    "    PyTorch no longer supports this GPU because it is too old.\n",
    "    The minimum cuda capability that we support is 3.5.\n",
    "</code>\n",
    "\n",
    "It would appear that I've purchased an inappropriate server for my work!  I currently have Pytorch 1.6.0 installed.  Forums say to drop back to 0.3.0 to get support for cuda capability 3.0.  That's a long way back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "Found 1 GPUs available. Using GPU 0 (Tesla K80) of compute capability 3.7 with 12.0Gb total memory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "device_id = torch.cuda.current_device()\n",
    "gpu_properties = torch.cuda.get_device_properties(device_id)\n",
    "print(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n",
    "      \"%.1fGb total memory.\\n\" % \n",
    "      (torch.cuda.device_count(),\n",
    "      device_id,\n",
    "      gpu_properties.name,\n",
    "      gpu_properties.major,\n",
    "      gpu_properties.minor,\n",
    "      gpu_properties.total_memory / 1e9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9/27/20:\n",
    "\n",
    "AWS says that\n",
    "\n",
    "* p3.2xlarge instance has one V100 GPU, which has compute capability of 7.0, plus 61 GB memory for $3.06/hr.\n",
    "\n",
    "* p2.xlarge instance has one K80 GPU, which provides compute capability of 3.7, plus 61 GB memory for $0.9/hr.\n",
    "\n",
    "* The g3 and g4 instances are intended for graphics-intense apps, and don't seem to provide an advantage here.\n",
    "\n",
    "I probably want to use the Deep Learning AMI (DLAMI), described here: https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html\n",
    "\n",
    "I want to use the Ubuntu 18.04 Conda variant.  I don't believe this is what I have on the 664 instance from class, so I'll create a new one, just to be sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10/24/20:\n",
    "\n",
    "Created a new EC2 instance, **ID = i-03ffe00145cb0b929**\n",
    "* Built from DLAMI Ubuntu 18.04 v35.0\n",
    "* Reuses security group created on 9/10, ID = sg-0b8d086b2fa6881a7 (launch-wizard-4), which is wide open on 5 ports.\n",
    "* Storage is EBS volume ID = vol-043c36a11deb7c7cd, which is 130 GB of gp2, and marked as delete on termination.\n",
    "    * The AMI takes up 74 GB\n",
    "    * The Udacity repo currently takes up 21 GB\n",
    "    * Total disk usage is 90 GB before I even start working.\n",
    "    * This storage persists between shutdowns of the instance.\n",
    "* Added .credentials file and user/security info to the .gitconfig file to enable pushing to the repo.\n",
    "\n",
    "* Found that torch & torchvision were not installed, despite the conda list being very long (and I thought the advertised DL AMI included pytorch).\n",
    "    * Resolved that by using command <code>conda install -c pytorch pytorch torchvision</code>\n",
    "    \n",
    "* **AI:** Tried to launch classroom instance (b664) as a comparison for network access configs, since it allowed me to access via https and have a terminal on a web page, which was handy since it is visually different from the normal Ubuntu VM terminals.  I currently cannot do this with my new machine, so I ssh via VM terminals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After false starts, training for real\n",
    "\n",
    "### First run (10/26/20)\n",
    "First real training run on AWS p2.xlarge with GPU:\n",
    "* Hyperparams: \n",
    "<code>\n",
    "    LR = 0.01\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.3, inplace=False)\n",
    "    )\n",
    "</code>\n",
    "* Result:  Never saw any training or validation loss decrease.  Stopped at 1.10 after epoch 6.\n",
    "\n",
    "### Run 2 (10/26/20)\n",
    "Adjusted LR and dropout.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.5, inplace=False)\n",
    "    )\n",
    "    Training on GPU\n",
    "    LR =  0.001\n",
    "</code>\n",
    "* Result:  Slightly better. Went 12 epochs, but ended with val loss = 1.08.\n",
    "\n",
    "### Run 3 (11/1/20)\n",
    "Adjusted LR\n",
    "* Hyperparams:\n",
    "<code>\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.5, inplace=False)\n",
    "    )\n",
    "    Training on GPU\n",
    "    LR =  0.0001\n",
    "</code>\n",
    "* Result:  Better still.  Best result: Epoch 30: training loss = 0.899543   validation loss = 1.024926*\n",
    "    * Need to add in checkpoint file saving so I don't have to start from scratch each time. This is a slow network to train.\n",
    "    * Need to add batchnorm2d; also consider doing the maxpool after every 2 layers.\n",
    "    \n",
    "### Run 4 (11/2/20)\n",
    "* Added batch normalizing to layers 2, 3, 4\n",
    "* Added storage of checkpoint files periodically\n",
    "\n",
    "* Hyperparams:\n",
    "<code>\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.5, inplace=False)\n",
    "    )\n",
    "    LR = 0.0001\n",
    "</code>\n",
    "\n",
    "* Result: No good.  First epoch gave train loss = 0.89166, val loss = 1.02633, then it got fluctuated around the same values for the next 5 epochs.\n",
    "    * Getting CUDA memory errors now, when I rerun the model.  Need to restart the jupyter server.  This might indicate there is a problem in the way I implemented the batchnorm.\n",
    "\n",
    "### Run 5 (11/2/20)\n",
    "* Adjusted learning rate, since it seems it may have been bouncing around a minimum hole.\n",
    "\n",
    "* Hyperparams:\n",
    "<code>\n",
    "    Creating a new, untrained model.\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.5, inplace=False)\n",
    "    )\n",
    "    Training on GPU\n",
    "    LR =  2e-05\n",
    "</code>\n",
    "\n",
    "* Result:  Crapped out right away after restarting jupyter server.  I definitely have a memory management problem in my code somewhere.  Will need to investigate later.\n",
    "\n",
    "### Run 6 (11/7/20)\n",
    "* Added memory reporting code from Udacity module 5 dog app.  Crapped out immediately in the backward() method, which suggests I am probably using the batchnorm incorrectly.\n",
    "* Removed all use of batchnorm, going back to the way it was for run 3.  Still got a rapid memory error after 4 batches.\n",
    "* Replaced separate calls to F.relu() to inline them as they were for run 3.  Memory climbed for the first 4 batches, then leveled out at 10.143 GB for 30 more batches. On the 35th batch cuda ran out of memory in the call to forward().\n",
    "\n",
    "### Run 7 (11/11/20)\n",
    "* Compared to the run 3 version (from commit 673f046e).  The only difference is the checkpoint file management.  So I commented this out and ran, even with the same learning rate as run 3, just to re-establish baseline performance.\n",
    "* Still running out of memory early in the first validation loop.  Added a bunch of memory reports throughout.\n",
    "* Realized I have been using batch size of 64, vice 32 used on run 3.  Returned to 32.\n",
    "* This ran 2 complete epochs and started a third, which looks like it might be able to finish. But I noticed that memory was still creeping up in between batches sometimes, so added garbage collection within each batch loop.  This helps a tremendous amount!  Ready to start rockin again.\n",
    "\n",
    "### Run 8 (11/11/20)\n",
    "* Changed learning rate.  Cleaned up memory debugging print statements.\n",
    "* Left the batchnorm statements commented out.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.5, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  2e-05\n",
    "</code>\n",
    "\n",
    "* Result:  consistently reducing loss over first 14 epochs, with constant memory use at end of each epoch (4.762 GB). I stopped it prematurely because it was learning very slowly and I now feel I understand how to apply batchnorm.\n",
    "\n",
    "### Run 9 (11/12/20)\n",
    "* Changed dropout from 0.5 to 0.2 (using batchnorm will make this less necessary)\n",
    "* Added batchnorm2d in 4th layer only.  I want to introduce it slowly to understand the implications on memory use.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  2e-05\n",
    "</code>\n",
    "    \n",
    "* Results:  \n",
    "    * Epoch 3 was lowest val loss (1.028), but it started decreasing more reapidly than before, at same LR.\n",
    "    * Memory use was 3.80 / 9.54 GB (max avaliable is 11.1 GB).\n",
    "    \n",
    "### Run 10 (11/14/20)\n",
    "* Improved checkpoint file naming & saving logic.\n",
    "* Adjusted LR up to 0.0001 based on documented improvements in learning with batch normalization.\n",
    "* Added batchnorm layer 3.  This isn't all I want to add, but want to gauge effect on memory use gradually.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  0.0001\n",
    "</code>\n",
    "\n",
    "* Result:  val loss was 1.03 on epoch 0, then increased from there.  Memory was mem 3.93/9.92 GB.\n",
    "\n",
    "### Run 11 (11/14/20)\n",
    "* Added layer 2 batchnorm.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  0.0001\n",
    "</code>\n",
    "\n",
    "* Result:  val loss was 1.03 on epoch 0, then increased from there.  Memory was 4.19/10.67 GB.\n",
    "\n",
    "### Run 12 (11/14/20)\n",
    "* Reducing batch size from 32 to 24 to avoid memory limits.\n",
    "* Changing learning rate smaller.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  1e-05\n",
    "</code>\n",
    "\n",
    "* Result:  val loss hit min of 1.02 on epoch 2. Model correctly saved.  Memory 2.76/9.24 GB.\n",
    "\n",
    "### Run 13 (11/14/20)\n",
    "* Added third FC classification layer. Set the layer sizes as equally distributed along the log scale between 98304 and 3.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=3082, bias=True)\n",
    "  (fc2): Linear(in_features=3082, out_features=96, bias=True)\n",
    "  (fc3): Linear(in_features=96, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  1e-05\n",
    "</code>\n",
    "\n",
    "* Result:  val loss dropped steadily down to 1.05 for first 12 epochs, where I stopped it in order to experiment with other stuff.  Memory use was 3.24/9.71 GB.\n",
    "\n",
    "### Run 14 (11/14/20)\n",
    "* Added batchnorm to conv layer 5\n",
    "* Reduced batch size to 20 (see results below).\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=3082, bias=True)\n",
    "  (fc2): Linear(in_features=3082, out_features=96, bias=True)\n",
    "  (fc3): Linear(in_features=96, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  1e-05\n",
    "</code>\n",
    "\n",
    "* Result: cuda ran out of memory immediately, so I reduced batch size to 20.  However, realized that it may have been a result of killing the previous run, leaving the gpu polluted.  Still, the extra batchnorm is adding some memory, so decided to be safe and leave it.\n",
    "    * Val loss got down to 1.02 on epoch 7.  Memory was 3.88/8.55 GB.\n",
    "* **Note:** There seems to be quite a debate about whether batchnorm should be performed before or after the activation function.  The original paper put it before.  But others seem to find that it is better after.  I agree with the principle of putting it before, since normalizing after the nonlinear activation feels like it is making a false adjustment on only the \"filtered\" outputs.  However, these are the actual outputs of the layer, which is the true input to the next layer.  And it seems that several people have, at least anecdotally, experienced that doing so after the activation provides improved performance.  See \n",
    "    * https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/\n",
    "    * https://forums.fast.ai/t/questions-about-batch-normalization/230/8\n",
    "\n",
    "### Run 15 (11/14/20)\n",
    "* Moved the batchnorm functions to after the relu functions in each of the conv layers 2-5.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=3082, bias=True)\n",
    "  (fc2): Linear(in_features=3082, out_features=96, bias=True)\n",
    "  (fc3): Linear(in_features=96, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  1e-05\n",
    "</code>\n",
    "\n",
    "* Result: Best epoch was #8, with losss of 1.0306, but epoch 10 showed 1.0308, the second-lowest, with a downward trend in training loss, so I thought it might start going down again.  Since I have a checkpoint of that one, I'd like to try continuing from there.  Memory was 4.10/8.57 GB.\n",
    "\n",
    "### Run 16 (11/15/20)\n",
    "* Continuing from run 15, reading from the checkpoint 8, but using a lower learning rate.  This is a test of starting from a checkpoint as much as anything.\n",
    "* New LR = 2e-6\n",
    "\n",
    "* Result:  val loss started out higher than the from run 15 where it left off (1.0493 vs 1.0306 on run 15).  Val loss intermittently found new lows, but the progress was not monotonic.  Even the training loss moves in waves, hitting a minimum, then retreating for 3 or 4 epochs before finding a new low.  Therefore, I think the termination criterion of > 6 epochs with no new low is too tight.\n",
    "\n",
    "\n",
    "\n",
    "**reminder:** increase number of epochs without decrease, and change comparison logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
