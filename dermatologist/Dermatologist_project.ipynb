{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist Sample Problem\n",
    "This is the optional problem provided in module 3 of the Udacity Deep Learning nanodegree program.  I am building it on my own, after completing the course.\n",
    "\n",
    "I'm trying to decide on a process flow.  Here I'll use the following phases:\n",
    "1. Project setup & acquire data\n",
    "2. Load & prepare the data (includes spot checking & sanity checking for cleanliness & appropriate content)\n",
    "3. Build the model\n",
    "4. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 1:** \n",
    "Data, provided by Udacity, already exists elsewhere in my local directory.  So this is probably a lot shorter than it would normally be.  This data set comes from [2017 ISIC Challenge on Skin Lesion Analysis Towards Melanoma Detection] (https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a).\n",
    "\n",
    "**Phase 2:**\n",
    "* These images are large, and vary in size.  Since I'm looking for lots of details to help improve classification, I don't want to throw away lots of info by unnecessarily croping or downsampling the images. Therefore, I first need to go through the data set and figure out the min/max sizes.  From there I can figure out what is possible.\n",
    "* Consisder using scaling like what is found in https://opensource.com/life/15/2/resize-images-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "import torch\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# returns the number of GiB of cuda memory used\n",
    "def memory_gb():\n",
    "    max_val = 0\n",
    "    if (train_on_gpu):\n",
    "        gb_alloc = torch.cuda.memory_allocated() / 1024 / 1024 / 1024\n",
    "        gb_res   = torch.cuda.memory_reserved() /  1024 / 1024 / 1024\n",
    "        max_val = gb_alloc\n",
    "        if gb_res > gb_alloc:\n",
    "            max_val = gb_res\n",
    "    return max_val\n",
    "\n",
    "# compares current memory use to a previous value & returns the larger\n",
    "def memory_max(prev_max):\n",
    "    mem = memory_gb()\n",
    "    if (mem > prev_max):\n",
    "        return mem\n",
    "    else:\n",
    "        return prev_max\n",
    "\n",
    "# printable report of cuda memory used\n",
    "def memory_rpt(label=\"\"):\n",
    "    print(\"      {}: mem {:.3f} GB\".format(label, memory_gb()))\n",
    "    \n",
    "# garbage collection\n",
    "def memory_clean():\n",
    "    if train_on_gpu:\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT_GOAL = 768 #pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n",
      "Using image size:  768\n",
      "Num training images:    2000\n",
      "Num validation images:  362\n",
      "Num test images:        388\n",
      "training data =  Dataset ImageFolder\n",
      "    Number of datapoints: 2000\n",
      "    Root location: /home/ubuntu/ml/dermatologist/data/train/\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomResizedCrop(size=(768, 768), scale=(0.8, 1.2), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
      "               ColorJitter(brightness=[0.8, 1.2], contrast=[0.8, 1.2], saturation=[0.9, 1.1], hue=None)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               RandomVerticalFlip(p=0.5)\n",
      "               RandomAffine(degrees=(-20, 20), translate=(0.1, 0.1))\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "\n",
    "# define training and test data directories\n",
    "if train_on_gpu:\n",
    "    data_dir = '/home/ubuntu/ml/dermatologist/data/' #for AWS server\n",
    "else:\n",
    "    data_dir = '/home/starkj/Udacity/repo/dermatologist/data/' #for local VM\n",
    "    \n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "val_dir   = os.path.join(data_dir, \"valid/\")\n",
    "test_dir  = os.path.join(data_dir, 'test/')\n",
    "\n",
    "# classes are folders in each directory with these names\n",
    "classes = ['melanoma', 'nevus', 'seborrheic_keratosis']\n",
    "\n",
    "# load and transform data using ImageFolder\n",
    "\n",
    "# load and transform data using ImageFolder\n",
    "### NOTE:  very important!  Some transforms work on PIL images and some on tensors.  Apply all the\n",
    "###        PIL transforms first, then convert the result to a Tensor, then we can apply further\n",
    "###        transforms if desired.\n",
    "\n",
    "image_size = HEIGHT_GOAL\n",
    "print(\"Using image size: \", image_size)\n",
    "\n",
    "full1 = transforms.Compose([\n",
    "                                     #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), #PIL only\n",
    "                                     transforms.RandomResizedCrop(image_size, scale=(0.8, 1.2)),   #PIL or Tensor\n",
    "                                     transforms.ColorJitter(brightness=0.2, contrast=0.2, #PIL or Tensor\n",
    "                                                            saturation=0.1, hue=0.0),\n",
    "                                     transforms.RandomHorizontalFlip(),                   #PIL or Tensor\n",
    "                                     transforms.RandomVerticalFlip(),                     #PIL or Tensor\n",
    "                                     transforms.RandomAffine(degrees=20, translate=(0.1, 0.1)), #PIL or Tensor\n",
    "                                     transforms.ToTensor()])\n",
    "no_xform = transforms.ToTensor()\n",
    "crop_only = transforms.Compose([transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "#####\n",
    "##### select the transform set here!\n",
    "#####\n",
    "data_transform = full1\n",
    "\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
    "val_data   = datasets.ImageFolder(val_dir, transform=data_transform)\n",
    "test_data  = datasets.ImageFolder(test_dir, transform=data_transform)\n",
    "\n",
    "# print out some data stats\n",
    "print('Num training images:   ', len(train_data))\n",
    "print('Num validation images: ', len(val_data))\n",
    "print('Num test images:       ', len(test_data))\n",
    "print('training data = ', train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration support\n",
    "class ImageDatasetStats:\n",
    "    def __init__(self):\n",
    "        self.min_w  = 99999\n",
    "        self.min_w2 = 99999 #second smallest\n",
    "        self.max_w  = 0\n",
    "        self.max_w2 = 0 #second largest\n",
    "        self.min_h  = 99999\n",
    "        self.min_h2 = 99999\n",
    "        self.max_h  = 0\n",
    "        self.max_h2 = 0\n",
    "\n",
    "    # accessors\n",
    "    def get_min_w(self):\n",
    "        return (self.min_w, self.min_w2)\n",
    "\n",
    "    def get_max_w(self):\n",
    "        return (self.max_w, self.max_w2)\n",
    "\n",
    "    def get_min_h(self):\n",
    "        return (self.min_h, self.min_h2)\n",
    "\n",
    "    def get_max_h(self):\n",
    "        return (self.max_h, self.max_h2)\n",
    "    \n",
    "    # set the first and/or second smallest min dimension if new item exceeds\n",
    "    def adjust_mins(self, min1, min2, val):\n",
    "        if val < min1:\n",
    "            min2 = min1\n",
    "            min1 = val\n",
    "        elif val < min2:\n",
    "            min2 = val\n",
    "        return (min1, min2)\n",
    "\n",
    "    # set the first and/or second largest max dimension if new item exceeds\n",
    "    def adjust_maxs(self, max1, max2, val):\n",
    "        if val > max1:\n",
    "            max2 = max1\n",
    "            max1 = val\n",
    "        elif val > max2:\n",
    "            max2 = val\n",
    "        return (max1, max2)\n",
    "\n",
    "    # iterate through an image dataset, finding the first and second largest and smallest values in each dimension\n",
    "    # (width and height)\n",
    "    #    data: ImageFolder\n",
    "    def find2dim_extremes(self, data):\n",
    "        for i in range(len(data)):\n",
    "            im = train_data.__getitem__(i)[0]\n",
    "            h = im.shape[1]\n",
    "            w = im.shape[2]\n",
    "            mins = self.adjust_mins(self.min_w, self.min_w2, w)\n",
    "            maxs = self.adjust_maxs(self.max_w, self.max_w2, w)\n",
    "            self.min_w  = mins[0]\n",
    "            self.min_w2 = mins[1]\n",
    "            self.max_w  = maxs[0]\n",
    "            self.max_w2 = maxs[1]\n",
    "            \n",
    "            mins = self.adjust_mins(self.min_h, self.min_h2, h)\n",
    "            maxs = self.adjust_maxs(self.max_h, self.max_h2, h)\n",
    "            self.min_h  = mins[0]\n",
    "            self.min_h2 = mins[1]\n",
    "            self.max_h  = maxs[0]\n",
    "            self.max_h2 = maxs[1]\n",
    "\n",
    "            if i%10 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "            #print(\"\\nimage shape = \", im.shape)\n",
    "            #print(\"w = \", w, \", h = \", h)\n",
    "            #print(\"width mins & maxes = \", self.min_w, self.min_w2, self.max_w, self.max_w2)\n",
    "            #print(\"height mins & maxs = \", self.min_h, self.min_h2, self.max_h, self.max_h2)\n",
    "   \n",
    "    # iterate through a dataset and count how many images are in each size bin, both height & width\n",
    "    # data: a DataSet of images\n",
    "    def generate_histogram(self, minw, maxw, minh, maxh, num_bins, data):\n",
    "        w_bin_width = int((maxw - minw)/num_bins)\n",
    "        h_bin_width = int((maxh - minh)/num_bins)\n",
    "        \n",
    "        # initialize the arrays that will count the number of items in each size bin\n",
    "        w_count = []\n",
    "        h_count = []\n",
    "        for i in range(num_bins):\n",
    "            w_count.append(0)\n",
    "            h_count.append(0)\n",
    "        \n",
    "        # determine the boundaries of each width bin\n",
    "        w_bin_upper_bound = []\n",
    "        for i in range(num_bins):\n",
    "            w_bin_upper_bound.append(minw + (i+1)*w_bin_width - 1)\n",
    "        w_bin_upper_bound[num_bins-1] = maxw #adjust size of final bin to account for bin width rounding\n",
    "        #print(\"w_bin_upper_bound = \", w_bin_upper_bound)\n",
    "        \n",
    "        # determine the boudnaries of each height bin\n",
    "        h_bin_upper_bound = []\n",
    "        for i in range(num_bins):\n",
    "            h_bin_upper_bound.append(minh + (i+1)*h_bin_width - 1)\n",
    "        h_bin_upper_bound[num_bins-1] = maxh #adjust size of final bin to account for bin width rounding\n",
    "        #print(\"h_bin_upper_bound = \", h_bin_upper_bound)\n",
    "        \n",
    "        # count the number of images that fall into each bin\n",
    "        dsize = len(data)\n",
    "        print(\"Binning images in dataset size \", dsize)\n",
    "        for i in range(dsize):\n",
    "            im = data.__getitem__(i)[0]\n",
    "            h = im.shape[1]\n",
    "            w = im.shape[2]\n",
    "            #print(\"Image \", i, \": h = \", h, \", w = \", w)\n",
    "\n",
    "            for j in range(num_bins):\n",
    "                if w <= w_bin_upper_bound[j]:\n",
    "                    w_count[j] += 1\n",
    "                    break\n",
    "            for j in range(num_bins):\n",
    "                if h <= h_bin_upper_bound[j]:\n",
    "                    h_count[j] += 1\n",
    "                    break\n",
    "        return w_count, h_count, w_bin_upper_bound, h_bin_upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nprint(\"train_data shape = \", type(train_data))\\n\\n# Make a loop to look at each image and determine range of sizes\\nstats = ImageDatasetStats()\\nstats.find2dim_extremes(train_data)\\nminw = stats.get_min_w()\\nmaxw = stats.get_max_w()\\nminh = stats.get_min_h()\\nmaxh = stats.get_max_h()\\nprint(\" \")\\nprint(\"Width extremes  = \", minw, maxw)\\nprint(\"Height extremes = \", minh, maxh)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### data exploration only\n",
    "'''\n",
    "\n",
    "print(\"train_data shape = \", type(train_data))\n",
    "\n",
    "# Make a loop to look at each image and determine range of sizes\n",
    "stats = ImageDatasetStats()\n",
    "stats.find2dim_extremes(train_data)\n",
    "minw = stats.get_min_w()\n",
    "maxw = stats.get_max_w()\n",
    "minh = stats.get_min_h()\n",
    "maxh = stats.get_max_h()\n",
    "print(\" \")\n",
    "print(\"Width extremes  = \", minw, maxw)\n",
    "print(\"Height extremes = \", minh, maxh)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling variable image sizes\n",
    "\n",
    "Clearly, from above, we have a wide range of image sizes, by almost 10x.  Cropping is not a good answer to this, because many of the images are framed tightly around the artifact of interest, so even a little cropping will be throwing away valuable information.  Other alternatives, which might be combined:\n",
    "\n",
    "1) Downsample the larger images - probably useful for a certain range of downsampling (e.g. up to 4x?) but we stand to lose too much info in the largest images if they are downsampled a lot.  See https://scikit-image.org/docs/dev/auto_examples/transform/plot_rescale.html\n",
    "\n",
    "2) Throw out the smallest and/or largest images as outliers - if there are only a few way out on the tails of the distribution, this could help reduce the magnitude of the problem, but the statistics above only show that there are at least two images at each of the very extremes.\n",
    "\n",
    "3) Build a network that handles variable image sizes - there are some techniques for this, but it involves advanced CNN construction, and adds difficulty to the final FC classification layer.  See:\n",
    "    * https://stats.stackexchange.com/questions/388859/is-it-possible-to-give-variable-sized-images-as-input-to-a-convolutional-neural\n",
    "    * https://www.reddit.com/r/MachineLearning/comments/akbe39/d_best_approach_to_variable_image_sizes_for_image/\n",
    "    * Thesis paper in Downloads folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstats = ImageDatasetStats()\\nres = stats.generate_histogram(576, 6748, 540, 4499, 11, train_data)\\n\\nprint(\"width counts = \", res[0])\\nprint(\"width Ubound = \", res[2])\\nprint(\"height counts =\", res[1])\\nprint(\"height Ubound =\", res[3])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### data exploration only\n",
    "'''\n",
    "stats = ImageDatasetStats()\n",
    "res = stats.generate_histogram(576, 6748, 540, 4499, 11, train_data)\n",
    "\n",
    "print(\"width counts = \", res[0])\n",
    "print(\"width Ubound = \", res[2])\n",
    "print(\"height counts =\", res[1])\n",
    "print(\"height Ubound =\", res[3])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on variable image sizes\n",
    "As the cell above shows, the distribution is far from Gaussian, with ~1/3 of all the images in the lowest size bin.  The upper four size bins combined are about 7% of the images in the training set.  Therefore, for the sake of what is supposed to be a fairly simple exercise here, I'll ignore images with w > 4502 or h > 3052 pixels.  For the remainder, I'll downsample the larger ones and pad the smaller ones.  Since most of the images are in the smallest bin, I don't want to perturb them too much.  The smallest being h=540 and w=576, it probably isn't prudent to pad more than 30%, so set my input image size to 700x748 pixels.  Anything smaller will be padded; anything larger will be downsampled and/or cropped.\n",
    "\n",
    "Problem is, the standard DataLoader won't accept input images that have variable sizes, per below, so need to figure out how to get around that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# This code fragment pulled from Udacity DLND module 3 homework.\\n# Note that it only works if all images are identical size (dataiter assumes this)\\n\\nimport matplotlib.pyplot as plt\\n\\n%matplotlib inline\\n\\n# define dataloader parameters\\nbatch_size = 32\\nnum_workers=0\\n\\n# prepare data loaders\\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \\n                                           num_workers=num_workers, shuffle=True)\\nval_loader   = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\\n                                           num_workers=num_workers, shuffle=True)\\ntest_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \\n                                           num_workers=num_workers, shuffle=True)\\n\\n# obtain one batch of training images\\ndataiter = iter(train_loader) ######### THIS LINE BREAKS - can\\'t handle variable image sizes\\nimages, labels = dataiter.next()\\nimages = images.numpy() # convert images to numpy for display\\nprint(\"images shape = \", images.shape)\\n\\n# plot the images in the batch, along with the corresponding labels\\nfig = plt.figure(figsize=(25, 4))\\nfor idx in np.arange(20):\\n    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\\n    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\\n    ax.set_title(classes[labels[idx]])\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### data exploration only - can't use this as is\n",
    "'''\n",
    "\n",
    "# This code fragment pulled from Udacity DLND module 3 homework.\n",
    "# Note that it only works if all images are identical size (dataiter assumes this)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# define dataloader parameters\n",
    "batch_size = 32\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader) ######### THIS LINE BREAKS - can't handle variable image sizes\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "print(\"images shape = \", images.shape)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading image data of variable sizes\n",
    "\n",
    "* There is an interesting answer for how to do this at https://discuss.pytorch.org/t/torchvision-and-dataloader-different-images-shapes/41026/3. Code from this page is copied below for ease of reading (without scroll bars).\n",
    "* Another, maybe more useful answer, with links to lots of other answers: https://stackoverflow.com/questions/55041080/how-does-pytorch-dataloader-handle-variable-size-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### for image exploration only\n",
    "### I don't think I want to use this, as it has masks that appear to be a substitute for resizing the images,\n",
    "### but it looks like it requires each image to be in a fixed size tensor, which I don't want to do.\n",
    "\n",
    "def default_collate(batch):\n",
    "    \"\"\"\n",
    "    Override `default_collate` https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader\n",
    "\n",
    "    Reference:\n",
    "    def default_collate(batch) at https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader\n",
    "    https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278/3\n",
    "    https://github.com/pytorch/pytorch/issues/1512\n",
    "\n",
    "    We need our own collate function that wraps things up (imge, mask, label).\n",
    "\n",
    "    In this setup,  batch is a list of tuples (the result of calling: img, mask, label = Dataset[i].\n",
    "    The output of this function is four elements:\n",
    "        . data: a pytorch tensor of size (batch_size, c, h, w) of float32 . Each sample is a tensor of shape (c, h_,\n",
    "        w_) that represents a cropped patch from an image (or the entire image) where: c is the depth of the patches (\n",
    "        since they are RGB, so c=3),  h is the height of the patch, and w_ is the its width.\n",
    "        . mask: a list of pytorch tensors of size (batch_size, 1, h, w) full of 1 and 0. The mask of the ENTIRE image (no\n",
    "        cropping is performed). Images does not have the same size, and the same thing goes for the masks. Therefore,\n",
    "        we can't put the masks in one tensor.\n",
    "        . target: a vector (pytorch tensor) of length batch_size of type torch.LongTensor containing the image-level\n",
    "        labels.\n",
    "    :param batch: list of tuples (img, mask, label)\n",
    "    :return: 3 elements: tensor data, list of tensors of masks, tensor of labels.\n",
    "    \"\"\"\n",
    "    data = torch.stack([item[0] for item in batch])\n",
    "    mask = [item[1] for item in batch]  # each element is of size (1, h*, w*). where (h*, w*) changes from mask to another.\n",
    "    target = torch.LongTensor([item[2] for item in batch])  # image labels.\n",
    "\n",
    "    return data, mask, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Defining a custom collator for the DataLoader since its default assumes all raw images are identical\n",
    "# size.  In our case they are not.  This function not only loads different size images, but manipulates\n",
    "# them to be identical size (downsampling and cropping), because the CNN being used in this project\n",
    "# is assuming that all images are identical size.\n",
    "# This function is modified from https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/\n",
    "#\n",
    "# `batch` is a list of tuple where first element is image tensor and\n",
    "# second element is corresponding label\n",
    "#\n",
    "# return is a tuple of (data, target), where data is a tensor of shape [b, c, h, w] and\n",
    "# target is a list of labels\n",
    "\n",
    "AR_GOAL = 1.333333 # represents 768x1024\n",
    "WIDTH_GOAL = HEIGHT_GOAL * AR_GOAL\n",
    "\n",
    "def variable_collate(batch):\n",
    "    data_list = []\n",
    "    for item in batch:\n",
    "        \n",
    "        # get info on the raw image shape\n",
    "        raw_image = item[0]\n",
    "        h = raw_image.shape[1]\n",
    "        w = raw_image.shape[2]\n",
    "        ar = w/h\n",
    "        \n",
    "        # if the aspect ratio is larger than desired, then we scale by the height, otherwise by width\n",
    "        if ar > AR_GOAL:\n",
    "            scale_factor = HEIGHT_GOAL / h\n",
    "        else:\n",
    "            scale_factor = WIDTH_GOAL / w\n",
    "        #print(\"In variable_collate: raw h = \", h, \", w = \", w, \", scale_factor = \", scale_factor)\n",
    "        #print(\"  raw_image = \", raw_image.shape)\n",
    "            \n",
    "        # convert the raw_image tensor into a PIL Image object\n",
    "        image = transforms.ToPILImage()(raw_image.squeeze_(0))\n",
    "        #print(\"  After transform, image = \", image.size)\n",
    "        \n",
    "        # scale it, preserving aspect ratio\n",
    "        resized = image.resize((int(w*scale_factor), int(h*scale_factor)))\n",
    "        width = resized.width\n",
    "        height = resized.height\n",
    "        #print(\"  resized w = \", width, \", h = \", height)\n",
    "            \n",
    "        # now we have an image that doesn't necessarily match our goal aspect ratio, but is at least\n",
    "        # as large as our goal dimensions in both directions; need to center-crop it - \n",
    "        # DON'T try to make it square!\n",
    "        if width > WIDTH_GOAL:\n",
    "            left = (width - WIDTH_GOAL) // 2\n",
    "        else:\n",
    "            left = 0\n",
    "        right = left + WIDTH_GOAL #don't subtract 1; this is the next pixel beyond the edge of the image\n",
    "        \n",
    "        if height > HEIGHT_GOAL:\n",
    "            upper = (height - HEIGHT_GOAL) // 2\n",
    "        else:\n",
    "            upper = 0\n",
    "        lower = upper + HEIGHT_GOAL #don't subtract 1\n",
    "        #print(\"  cropping to left = \", left, \", right = \", right, \", upper = \", upper, \", lower = \", lower)\n",
    "        \n",
    "        cropped_image = resized.crop((left, upper, right, lower))\n",
    "        \n",
    "        # convert the properly sized image back to a tensor\n",
    "        cropped_tensor = transforms.ToTensor()(cropped_image).unsqueeze_(0)\n",
    "        data_list.append(cropped_tensor)\n",
    "        #print(\"  cropped_tensor = \", cropped_tensor.shape)\n",
    "        \n",
    "\n",
    "    # assemble all of the images into the data tensor for output\n",
    "    data = torch.stack(data_list).squeeze()\n",
    "    #print(\"  returning data = \", data.shape)\n",
    "    \n",
    "    # gather the target labels for each image\n",
    "    target = [item[1] for item in batch]\n",
    "    target = torch.LongTensor(target)\n",
    "    \n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader parameters\n",
    "batch_size = 32\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, collate_fn=variable_collate, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_data, collate_fn=variable_collate, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, collate_fn=variable_collate, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images type from dataiter =  <class 'torch.Tensor'>\n",
      "images shape =  (3, 768, 1024)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bb80e29a50fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#num rows, num images/row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \"\"\"\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transpose'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKYAAABwCAYAAACU7wtFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAABi0lEQVR4nO3VwU0DUQxAwf2IEjZntv9akiJyhh5MA4lEJAJPyszVPvjwJK+Z2aDm7b8PgFuESZIwSRImScIkSZgkvT+yvO/7HMfxpFN4NZfL5WtmTrdmD4V5HMd2Pp9/5ype3lrrem/mlZMkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSRImScIkSZgkCZMkYZIkTJKESZIwSVoz8/PltT63bbs+7xxezMfMnG4NHgoT/opXTpIwSRImScIkSZgkCZMkYZIkTJKESdI3EKcagQ3JdNMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1800x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### For image exploration only\n",
    "\n",
    "# This code fragment pulled from Udacity DLND module 3 homework.\n",
    "# Note that it only works if all images are identical size (dataiter assumes this)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# obtain one batch of training images - these need to be in a tensor of shape [b, c, w, h]\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"images type from dataiter = \", type(images))\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "print(\"images shape = \", images.shape)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 8)) #numbers indicate horiz & vert spacing between images\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(4, 32/4, idx+1, xticks=[], yticks=[]) #num rows, num images/row\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images look pretty good!  Sizes of the objects of interest aren't as uniform as I'd like, but for the purposes of this exercise, I think close enough.  To get better, it's probably worth doing some pre-processing with some sort of edge/feature detection to define a bounding box in the raw image, then crop around that before doing the scaling I've done here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize validation & testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation\n",
    "\n",
    "# obtain one batch of training images - these need to be in a tensor of shape [b, c, w, h]\n",
    "dataiter = iter(val_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "print(\"Validation samples\")\n",
    "fig = plt.figure(figsize=(25, 8)) #numbers indicate horiz & vert spacing between images\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(4, 32/4, idx+1, xticks=[], yticks=[]) #num rows, num images/row\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test\n",
    "\n",
    "# obtain one batch of training images - these need to be in a tensor of shape [b, c, w, h]\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "print(\"\\nTest samples\")\n",
    "fig = plt.figure(figsize=(25, 8)) #numbers indicate horiz & vert spacing between images\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(4, 32/4, idx+1, xticks=[], yticks=[]) #num rows, num images/row\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images look reasonably similar to the training images, ensuring that the validation & test processes will be subjected to the same kinds of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model\n",
    "\n",
    "I would like to use transfer learning with one of the big CNN's, but I need one that will use my chosen image size, or something close to it.\n",
    "* VGG uses 224x224\n",
    "* ResNet50 - not clear, but it looks like it can handle different sizes, up to 640 on short side.\n",
    "* GoogLeNet 50 appears to take 224x224 also.\n",
    "\n",
    "I'm starting to think all of these models are built for 224 square as a de facto standard for benchmarking.  Therefore, I choose to write my own model from scratch.  There is no doubt it will perform much worse than these world class models, but it will have one advantage in being able to use more input data (more pixels in the input images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkinCancerCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkinCancerCnn, self).__init__()\n",
    "        \n",
    "        #self.CONV_OUT_FEATURES = int(128 * (768/2/2/2/2/2) * (1024/2/2/2/2/2)) #for 5 conv layers\n",
    "        self.CONV_OUT_FEATURES = int(64 * (768/2/2/2/2) * (1024/2/2/2/2)) #for 4 conv layers\n",
    "        print(\"CONV_OUT_FEATURES = \", self.CONV_OUT_FEATURES)\n",
    "        \n",
    "        # Applying batch norm to all but first and last layers, per guidance from the DL nanodegree\n",
    "        # class, module 5.  Not clear why this restriction, however.\n",
    "        \n",
    "        #input images are 768x1024 pixels, full color (3 channels)\n",
    "        self.c1 = nn.Conv2d(3, 8, 4, 2, padding=1) #stride = 2\n",
    "        #avoid using batchnorm on first layer; not clear why, but that was important in Udacity lesson 5.\n",
    "\n",
    "        self.c2 = nn.Conv2d(8, 16, 4, 2, padding=1, bias=False)\n",
    "        self.b2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.c3 = nn.Conv2d(16, 32, 4, 2, padding=1, bias=False)\n",
    "        self.b3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.c4 = nn.Conv2d(32, 64, 4, 2, padding=1, bias=False) #must not use bias if using batchnorm!\n",
    "        self.b4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        #self.c5 = nn.Conv2d(64, 128, 4, 2, padding=1, bias=False)\n",
    "        #self.b5 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        #self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        #linear classifier\n",
    "        self.fc1 = nn.Linear(self.CONV_OUT_FEATURES, 3082)\n",
    "        self.fc2 = nn.Linear(3082, 96)\n",
    "        self.fc3 = nn.Linear(96, 3)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #memory_rpt(\"forward top\")\n",
    "        #print(\"      at entry, x.shape = \", x.shape)\n",
    "        \n",
    "        # conv layer 1 - output 8 x 384 x 512\n",
    "        x = F.leaky_relu(self.c1(x))\n",
    "        #x = self.pool(x)\n",
    "        \n",
    "        # Experimenting with placing the BN function before or after the relu in each layer.\n",
    "        # It seems there is some debate about which is best.\n",
    "        \n",
    "        # conv layer 2 - output 16 x 192 x 256\n",
    "        x = self.c2(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.05)\n",
    "        x = self.b2(x)\n",
    "        #x = self.pool(x)\n",
    "        \n",
    "        # conv layer 3 - output 32 x 96 x 128\n",
    "        x = self.c3(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.05)\n",
    "        x = self.b3(x)\n",
    "        #x = self.pool(x)\n",
    "        \n",
    "        # conv layer 4 - output 64 x 48 x 64\n",
    "        x = self.c4(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.05)\n",
    "        x = self.b4(x)\n",
    "        #x = self.pool(x)\n",
    "        \n",
    "        # conv layer 5\n",
    "        #x = self.c5(x)         #output 128 x 48 x 64\n",
    "        #x = F.leaky_relu(x, negative_slope=0.05)\n",
    "        #x = self.b5(x)\n",
    "        #x = self.pool(x)       #output 128 x 24 x 32\n",
    "        \n",
    "        ### CONSIDER putting garbage collectiion in between these layers - worth some experimenting.\n",
    "        #memory_rpt(\"  fwd conv \")\n",
    "        \n",
    "        conv_out = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        #print(\"      after conv, x.shape = \", x.shape, \"conv_out = \", conv_out)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1) #flatten the convolved image\n",
    "        #memory_rpt(\"  fwd view \")\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.05)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=0.05)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # softmax will be applied during training (as part of loss function), not here. But need\n",
    "        # output of the final layer to be positive, so don't use leaky relu.\n",
    "        x = F.relu(self.fc3(x)) \n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "best_checkpoint_file = \"Empty\" #global variable for use in testing\n",
    "\n",
    "# Trains the model\n",
    "# run_id:          identifier used in checkpoint file naming\n",
    "# prev_run:        identifier used in file naming of previous checkpoint to be read at beginning\n",
    "# starting_point:  input ID of the checkpoint file to use as a starting point (0 = no checkpoint)\n",
    "# num_epochs:      number of epochs to run in this call\n",
    "# save_interval:   save a checkpoint every this many epochs\n",
    "# learn_rate:      the (constant) learning rate applied to the training loop\n",
    "\n",
    "def train_it(run_id=\"XXX\", prev_run=\"XXX\", starting_point=0, num_epochs=10, save_interval=10, learn_rate=0.001):\n",
    "    \n",
    "    min_val_loss = 999999.0\n",
    "    min_val_epoch = 0\n",
    "    val_retry_limit = 10\n",
    "    file_prefix = \"model/derma\"\n",
    "\n",
    "    if starting_point == 0:\n",
    "        print(\"Creating a new, untrained model.\")\n",
    "        model = SkinCancerCnn()\n",
    "    else:\n",
    "        checkpoint_file = \"{}{}-{:03d}.pt\".format(file_prefix, prev_run, starting_point)\n",
    "        print(\"Retrieving partially trained model from \", checkpoint_file)\n",
    "        model = torch.load(checkpoint_file)\n",
    "\n",
    "    # define expected probabilities of each class based on labels across all train/val/test data\n",
    "    weights = torch.tensor([0.19, #melanoma\n",
    "                            0.67, #nevus\n",
    "                            0.14])#sebhorreic keratosis\n",
    "\n",
    "    print(model)\n",
    "    print(\"LR = \", learn_rate)\n",
    "    if train_on_gpu:\n",
    "        model = model.cuda()\n",
    "        weights = weights.cuda()\n",
    "        print(\"Training on GPU\")\n",
    "\n",
    "    # define criterion & optimizer.  CAUTION: must be done after moving model to gpu, if it is used.\n",
    "    criterion = nn.CrossEntropyLoss(weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learn_rate)\n",
    "\n",
    "    print(\"Entering training/validation epoch loop.\")\n",
    "    #memory_rpt()\n",
    "    max_mem = memory_gb()\n",
    "    start_time = time.perf_counter() #was time.clock()\n",
    "\n",
    "    for epoch in range(starting_point, starting_point+num_epochs):\n",
    "        \n",
    "        ###############\n",
    "        # training loop\n",
    "        ###############\n",
    "        \n",
    "        model.train() #put it into training mode\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            if train_on_gpu:\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            #print(\"output = \", output)\n",
    "            #print(\"target = \", target)\n",
    "\n",
    "            #memory_rpt(\"  after model run\")\n",
    "            max_mem = memory_max(max_mem)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()*data.size(0) #CrossEntropy already divides by num items in batch\n",
    "\n",
    "            # sanity check samples from each class\n",
    "            class_count = np.zeros(3, dtype=int)\n",
    "            tgt_count = np.zeros(3, dtype=int)\n",
    "            pred        = output.data.max(1, keepdim=True)[1].numpy().astype(int)\n",
    "            pred_values = output.data.max(1, keepdim=True)[0]\n",
    "            for item in pred:\n",
    "                class_count[item] += 1\n",
    "            for item in target.cpu():\n",
    "                tgt_count[item] += 1\n",
    "            sum = 0.0\n",
    "            for item in pred_values:\n",
    "                sum += item\n",
    "            avg_val = sum / len(pred_values)\n",
    "            #print(\"       Train batch: class_count = \", class_count, \", tgt_count = \", tgt_count,\n",
    "            #      \", avg class value = {:.3f}\".format(avg_val[0].item()))\n",
    "            #print(\"          Raw output:\")\n",
    "            #print(output)\n",
    "\n",
    "            # ensure garbage doesn't fill up the gpu memory\n",
    "            memory_clean()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        #################\n",
    "        # validation loop\n",
    "        #################\n",
    "        \n",
    "        # validate the training on this epoch\n",
    "        #print(\"    // Begin validation //\")\n",
    "        with torch.no_grad(): # may improve performance, but probably not calc results\n",
    "            model.eval() #put into evaluation mode\n",
    "            val_loss = 0.0\n",
    "            class_count = np.zeros(3, dtype=int)\n",
    "            tgt_count   = np.zeros(3, dtype=int)\n",
    "            class_correct = np.zeros(3, dtype=int)\n",
    "           \n",
    "            for data, target in val_loader:\n",
    "                if train_on_gpu:\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    model = model.cuda()     ### do we need this?  If so, do it in train loop above also.\n",
    "\n",
    "                output = model(data)\n",
    "                #memory_rpt(\"  after VAL model run\")\n",
    "                max_mem = memory_max(max_mem)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()*data.size(0)\n",
    "\n",
    "                # check the correctness statistics on the various classes - results printed w/epoch stats\n",
    "                pred        = output.data.max(1, keepdim=True)[1].numpy().astype(int)\n",
    "                pred_values = output.data.max(1, keepdim=True)[0]\n",
    "                for item in pred:\n",
    "                    class_count[item] += 1\n",
    "                for item in target:\n",
    "                    tgt_count[item] += 1\n",
    "                sum = 0.0\n",
    "                for item in pred_values:\n",
    "                    sum += item\n",
    "                avg_val = sum / len(pred_values)\n",
    "                #print(\"       Val batch: class_count = \", class_count, \", tgt_count = \", tgt_count,\n",
    "                #      \", avg class value = {:.3f}\".format(avg_val[0].item()))\n",
    "                #print(\"          Raw output:\")\n",
    "                #print(output)\n",
    "                for i in range(len(pred)):\n",
    "                    p = pred[i].item()\n",
    "                    #print(\"          i = {}, pred = {}, tgt = {}\".format(i, p, target[i]))\n",
    "                    if p == target[i]:\n",
    "                        class_correct[p] += 1\n",
    "\n",
    "                # ensure garbage doesn't fill up the gpu memory\n",
    "                memory_clean()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        #############################\n",
    "        # collect & print epoch stats\n",
    "        #############################\n",
    "\n",
    "        # update the validation loss statistics\n",
    "        flag = ' ' \n",
    "        if val_loss <= min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            min_val_epoch = epoch\n",
    "            flag = '*'\n",
    "            \n",
    "            # save a copy of the model\n",
    "            best_model = model\n",
    "            \n",
    "        # determine epoch duration and estimate remaining time\n",
    "        current_time = time.perf_counter() #was time.clock()\n",
    "        avg_duration = (current_time - start_time) / (epoch - starting_point + 1) / 60.0 #minutes\n",
    "        remaining_time_minutes = (starting_point + num_epochs - epoch - 1) * avg_duration\n",
    "        if remaining_time_minutes > 80:\n",
    "            rem_time = remaining_time_minutes / 60.0\n",
    "            time_est_msg = \"{:4.1f} hr rem.\".format(rem_time)\n",
    "        else:\n",
    "            time_est_msg = \"{:.0f} min rem.\".format(remaining_time_minutes)\n",
    "\n",
    "\n",
    "        # show epoch results\n",
    "        max_mem = memory_max(max_mem)\n",
    "        memory_clean()\n",
    "        print(\"///// Epoch {:3d}: train = {:.4f}, val = {:.4f}{}, mem {:.2f}/{:.2f} GB. Avg {:.1f} min/epoch; {}\".format(\n",
    "            epoch, train_loss, val_loss, flag, memory_gb(), max_mem, avg_duration, time_est_msg))\n",
    "        print(\"      Val class counts: \", class_count, \", target counts: \", tgt_count,\n",
    "              \", avg class value = {:.3f}\".format(avg_val[0].item()))\n",
    "        print(\"      Val accuracies:\")\n",
    "        for i in range(len(class_count)):\n",
    "            print(\"          Class {:d}:  {:5.1f}% ({:d}/{:d})\".format(i, 100.0*class_correct[i]/tgt_count[i],\n",
    "                                                                  class_correct[i], tgt_count[i]))\n",
    "\n",
    "        # save a checkpoint if it's appropriate\n",
    "        if epoch > 0  and  epoch % save_interval == 0:\n",
    "            save_file = \"{}{}-{:03d}.pt\".format(file_prefix, run_id, epoch)\n",
    "            print(\"      Saving checkpoint \", save_file)\n",
    "            torch.save(model, save_file)\n",
    "        \n",
    "        # determine if training is complete, based on validation loss hitting a minimum recently\n",
    "        if epoch >= (min_val_epoch + val_retry_limit):\n",
    "            print(\"      Training terminated due to no further decrease in validation loss.\")\n",
    "            break\n",
    "            \n",
    "    # save a checkpoint of the best epoch\n",
    "    save_file = \"{}{}-{:03d}.pt\".format(file_prefix, run_id, min_val_epoch)\n",
    "    print(\"      Saving best checkpoint \", save_file)\n",
    "    torch.save(best_model, save_file)\n",
    "    best_checkpoint_file = save_file\n",
    "\n",
    "    print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new, untrained model.\n",
      "CONV_OUT_FEATURES =  196608\n",
      "SkinCancerCnn(\n",
      "  (c1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (c2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (c4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=196608, out_features=3082, bias=True)\n",
      "  (fc2): Linear(in_features=3082, out_features=96, bias=True)\n",
      "  (fc3): Linear(in_features=96, out_features=3, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "LR =  1e-06\n",
      "Training on GPU\n",
      "Entering training/validation epoch loop.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [8, 3, 4, 4], but got 3-dimensional input of size [3, 768, 1024] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1a3fb2d65643>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprev_run_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRUN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprev_run_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.000001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-f5b74bea40f9>\u001b[0m in \u001b[0;36mtrain_it\u001b[0;34m(run_id, prev_run, starting_point, num_epochs, save_interval, learn_rate)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;31m#print(\"output = \", output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m#print(\"target = \", target)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f231df285c7d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# conv layer 1 - output 8 x 384 x 512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#x = self.pool(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [8, 3, 4, 4], but got 3-dimensional input of size [3, 768, 1024] instead"
     ]
    }
   ],
   "source": [
    "\n",
    "RUN      = \"20\"  ##### set this before each run!\n",
    "prev_run = \"\"\n",
    "prev_run_start = 0\n",
    "\n",
    "train_it(RUN, prev_run, starting_point=prev_run_start, num_epochs=50, learn_rate=0.000001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Pull in the checkpoint file that performed the best above and run it on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is adapted from my homework on the Udacity dog_app\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def test_it(test_file=None):\n",
    "    \n",
    "    # load the file\n",
    "    if test_file == None:\n",
    "        model = torch.load(best_checkpoint_file)\n",
    "    else:\n",
    "        model = torch.load(test_file)\n",
    "    \n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    correct_by_class = np.zeros(3)\n",
    "    print(\"correct_by_class = \", correct_by_class)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        # move to GPU\n",
    "        if train_on_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # update average test loss \n",
    "        test_loss += (1 / (batch_idx + 1)) * (loss.data - test_loss)\n",
    "        # convert output probabilities to predicted class - max() here returns a tuple of (values, indices),\n",
    "        #     where values is a tensor of size [batch_size, 3] holding the predictions of each of the three\n",
    "        #     output classes.  incides is a tensor of size [batch_size] holding the index of the max value \n",
    "        #     in each row, which corresponds to the winning class.\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # compare predictions to true label\n",
    "        print(\"output = \", output)\n",
    "        \n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.5f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (100.0 * correct / total, correct, total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_file = \"model/derma17-026.pt\"\n",
    "\n",
    "test_it(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AWS\n",
    "Ran the above code for 2 epochs on the local computer to determine that the model is functional and seems to be trainable (loss decreased).\n",
    "\n",
    "Now it's time to move it to an AWS instance in order to train.  I discovered my existing instance does not have any storage attached (I had been using a separate EBS volume, and deleted it after class).  Therefore, choosing a new instance:  <code>g2.2xlarge</code>, which has 1 GPU and 2 cores of CPU (2 threads each) and 16 GB of memory.\n",
    "\n",
    "System volume is /dev/sda1.  Working data storage is on /dev/sdb (60 GB).  Instance ID is i-0be255fe5cb1becf1.  Username is default \"ubuntu\".  Changed pwd to H1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Ubuntu VM disk size problem\n",
    "\n",
    "Diversion for a few days...\n",
    "\n",
    "Got into trouble by filling up my hard drive.  Took a long time to move data off and back it up, then figure out how to extend the drive space in VMWare.  \n",
    "* First, shut down the VM, then in Settings select Extend drive space.  This enlarges the physical drive only.  It doesn't extend the partition or the file system in the partition.\n",
    "* Next use gparted to enlarge the physical partition (I did not have a logical volume, so LVM was not an option).  Decent instructions here to get started, then just follow intuitin and the UI:  https://unix.stackexchange.com/questions/196512/how-to-extend-filesystem-partition-on-ubuntu-vm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transfer\n",
    "\n",
    "Now that VM drive space problem is solved, I re-downloaded all of the data from the original Udacity problem and moved it into my Github repo under dermatologist/data.  This took quite a while, as each push has to complete within a certain timeout period (not clear to me what causes that, but it appears it's my host screen saver time that essentially shuts down VM activity).\n",
    "\n",
    "Attempted to pull the entire dermatologist branch down to my AWS server, but ran out of drive space there. Cleaned it up and pulled all the data.  Now ready to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "\n",
    "### Error using GPU\n",
    "Upon first run, I got the following error:\n",
    "\n",
    "<code>\n",
    "/home/ubuntu/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:102: UserWarning: \n",
    "    Found GPU0 GRID K520 which is of cuda capability 3.0.\n",
    "    PyTorch no longer supports this GPU because it is too old.\n",
    "    The minimum cuda capability that we support is 3.5.\n",
    "</code>\n",
    "\n",
    "It would appear that I've purchased an inappropriate server for my work!  I currently have Pytorch 1.6.0 installed.  Forums say to drop back to 0.3.0 to get support for cuda capability 3.0.  That's a long way back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "device_id = torch.cuda.current_device()\n",
    "gpu_properties = torch.cuda.get_device_properties(device_id)\n",
    "print(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n",
    "      \"%.1fGb total memory.\\n\" % \n",
    "      (torch.cuda.device_count(),\n",
    "      device_id,\n",
    "      gpu_properties.name,\n",
    "      gpu_properties.major,\n",
    "      gpu_properties.minor,\n",
    "      gpu_properties.total_memory / 1e9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9/27/20:\n",
    "\n",
    "AWS says that\n",
    "\n",
    "* p3.2xlarge instance has one V100 GPU, which has compute capability of 7.0, plus 61 GB memory for $3.06/hr.\n",
    "\n",
    "* p2.xlarge instance has one K80 GPU, which provides compute capability of 3.7, plus 61 GB memory for $0.9/hr.\n",
    "\n",
    "* The g3 and g4 instances are intended for graphics-intense apps, and don't seem to provide an advantage here.\n",
    "\n",
    "I probably want to use the Deep Learning AMI (DLAMI), described here: https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html\n",
    "\n",
    "I want to use the Ubuntu 18.04 Conda variant.  I don't believe this is what I have on the 664 instance from class, so I'll create a new one, just to be sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10/24/20:\n",
    "\n",
    "Created a new EC2 instance, **ID = i-03ffe00145cb0b929**\n",
    "* Built from DLAMI Ubuntu 18.04 v35.0\n",
    "* Reuses security group created on 9/10, ID = sg-0b8d086b2fa6881a7 (launch-wizard-4), which is wide open on 5 ports.\n",
    "* Storage is EBS volume ID = vol-043c36a11deb7c7cd, which is 130 GB of gp2, and marked as delete on termination.\n",
    "    * The AMI takes up 74 GB\n",
    "    * The Udacity repo currently takes up 21 GB\n",
    "    * Total disk usage is 90 GB before I even start working.\n",
    "    * This storage persists between shutdowns of the instance.\n",
    "* Added .credentials file and user/security info to the .gitconfig file to enable pushing to the repo.\n",
    "\n",
    "* Found that torch & torchvision were not installed, despite the conda list being very long (and I thought the advertised DL AMI included pytorch).\n",
    "    * Resolved that by using command <code>conda install -c pytorch pytorch torchvision</code>\n",
    "    \n",
    "* **AI:** Tried to launch classroom instance (b664) as a comparison for network access configs, since it allowed me to access via https and have a terminal on a web page, which was handy since it is visually different from the normal Ubuntu VM terminals.  I currently cannot do this with my new machine, so I ssh via VM terminals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### After false starts, training for real\n",
    "\n",
    "### First run (10/26/20)\n",
    "First real training run on AWS p2.xlarge with GPU:\n",
    "* Hyperparams: \n",
    "<code>\n",
    "    LR = 0.01\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.3, inplace=False)\n",
    "    )\n",
    "</code>\n",
    "* Result:  Never saw any training or validation loss decrease.  Stopped at 1.10 after epoch 6.\n",
    "\n",
    "### Run 2 (10/26/20)\n",
    "Adjusted LR and dropout.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.5, inplace=False)\n",
    "    )\n",
    "    Training on GPU\n",
    "    LR =  0.001\n",
    "</code>\n",
    "* Result:  Slightly better. Went 12 epochs, but ended with val loss = 1.08.\n",
    "\n",
    "### Run 3 (11/1/20)\n",
    "Adjusted LR\n",
    "* Hyperparams:\n",
    "<code>\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.5, inplace=False)\n",
    "    )\n",
    "    Training on GPU\n",
    "    LR =  0.0001\n",
    "</code>\n",
    "* Result:  Better still.  Best result: Epoch 30: training loss = 0.899543   validation loss = 1.024926*\n",
    "    * Need to add in checkpoint file saving so I don't have to start from scratch each time. This is a slow network to train.\n",
    "    * Need to add batchnorm2d; also consider doing the maxpool after every 2 layers.\n",
    "    \n",
    "### Run 4 (11/2/20)\n",
    "* Added batch normalizing to layers 2, 3, 4\n",
    "* Added storage of checkpoint files periodically\n",
    "\n",
    "* Hyperparams:\n",
    "<code>\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.5, inplace=False)\n",
    "    )\n",
    "    LR = 0.0001\n",
    "</code>\n",
    "\n",
    "* Result: No good.  First epoch gave train loss = 0.89166, val loss = 1.02633, then it got fluctuated around the same values for the next 5 epochs.\n",
    "    * Getting CUDA memory errors now, when I rerun the model.  Need to restart the jupyter server.  This might indicate there is a problem in the way I implemented the batchnorm.\n",
    "\n",
    "### Run 5 (11/2/20)\n",
    "* Adjusted learning rate, since it seems it may have been bouncing around a minimum hole.\n",
    "\n",
    "* Hyperparams:\n",
    "<code>\n",
    "    Creating a new, untrained model.\n",
    "    CONV_OUT_FEATURES =  98304\n",
    "    SkinCancerCnn(\n",
    "      (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "      (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "      (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "      (dropout): Dropout(p=0.5, inplace=False)\n",
    "    )\n",
    "    Training on GPU\n",
    "    LR =  2e-05\n",
    "</code>\n",
    "\n",
    "* Result:  Crapped out right away after restarting jupyter server.  I definitely have a memory management problem in my code somewhere.  Will need to investigate later.\n",
    "\n",
    "### Run 6 (11/7/20)\n",
    "* Added memory reporting code from Udacity module 5 dog app.  Crapped out immediately in the backward() method, which suggests I am probably using the batchnorm incorrectly.\n",
    "* Removed all use of batchnorm, going back to the way it was for run 3.  Still got a rapid memory error after 4 batches.\n",
    "* Replaced separate calls to F.relu() to inline them as they were for run 3.  Memory climbed for the first 4 batches, then leveled out at 10.143 GB for 30 more batches. On the 35th batch cuda ran out of memory in the call to forward().\n",
    "\n",
    "### Run 7 (11/11/20)\n",
    "* Compared to the run 3 version (from commit 673f046e).  The only difference is the checkpoint file management.  So I commented this out and ran, even with the same learning rate as run 3, just to re-establish baseline performance.\n",
    "* Still running out of memory early in the first validation loop.  Added a bunch of memory reports throughout.\n",
    "* Realized I have been using batch size of 64, vice 32 used on run 3.  Returned to 32.\n",
    "* This ran 2 complete epochs and started a third, which looks like it might be able to finish. But I noticed that memory was still creeping up in between batches sometimes, so added garbage collection within each batch loop.  This helps a tremendous amount!  Ready to start rockin again.\n",
    "\n",
    "### Run 8 (11/11/20)\n",
    "* Changed learning rate.  Cleaned up memory debugging print statements.\n",
    "* Left the batchnorm statements commented out.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.5, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  2e-05\n",
    "</code>\n",
    "\n",
    "* Result:  consistently reducing loss over first 14 epochs, with constant memory use at end of each epoch (4.762 GB). I stopped it prematurely because it was learning very slowly and I now feel I understand how to apply batchnorm.\n",
    "\n",
    "### Run 9 (11/12/20)\n",
    "* Changed dropout from 0.5 to 0.2 (using batchnorm will make this less necessary)\n",
    "* Added batchnorm2d in 4th layer only.  I want to introduce it slowly to understand the implications on memory use.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  2e-05\n",
    "</code>\n",
    "    \n",
    "* Results:  \n",
    "    * Epoch 3 was lowest val loss (1.028), but it started decreasing more reapidly than before, at same LR.\n",
    "    * Memory use was 3.80 / 9.54 GB (max avaliable is 11.1 GB).\n",
    "    \n",
    "### Run 10 (11/14/20)\n",
    "* Improved checkpoint file naming & saving logic.\n",
    "* Adjusted LR up to 0.0001 based on documented improvements in learning with batch normalization.\n",
    "* Added batchnorm layer 3.  This isn't all I want to add, but want to gauge effect on memory use gradually.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  0.0001\n",
    "</code>\n",
    "\n",
    "* Result:  val loss was 1.03 on epoch 0, then increased from there.  Memory was mem 3.93/9.92 GB.\n",
    "\n",
    "### Run 11 (11/14/20)\n",
    "* Added layer 2 batchnorm.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  0.0001\n",
    "</code>\n",
    "\n",
    "* Result:  val loss was 1.03 on epoch 0, then increased from there.  Memory was 4.19/10.67 GB.\n",
    "\n",
    "### Run 12 (11/14/20)\n",
    "* Reducing batch size from 32 to 24 to avoid memory limits.\n",
    "* Changing learning rate smaller.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=1024, bias=True)\n",
    "  (fc2): Linear(in_features=1024, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  1e-05\n",
    "</code>\n",
    "\n",
    "* Result:  val loss hit min of 1.02 on epoch 2. Model correctly saved.  Memory 2.76/9.24 GB.\n",
    "\n",
    "### Run 13 (11/14/20)\n",
    "* Added third FC classification layer. Set the layer sizes as equally distributed along the log scale between 98304 and 3.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=3082, bias=True)\n",
    "  (fc2): Linear(in_features=3082, out_features=96, bias=True)\n",
    "  (fc3): Linear(in_features=96, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  1e-05\n",
    "</code>\n",
    "\n",
    "* Result:  val loss dropped steadily down to 1.05 for first 12 epochs, where I stopped it in order to experiment with other stuff.  Memory use was 3.24/9.71 GB.\n",
    "\n",
    "### Run 14 (11/14/20)\n",
    "* Added batchnorm to conv layer 5\n",
    "* Reduced batch size to 20 (see results below).\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=3082, bias=True)\n",
    "  (fc2): Linear(in_features=3082, out_features=96, bias=True)\n",
    "  (fc3): Linear(in_features=96, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  1e-05\n",
    "</code>\n",
    "\n",
    "* Result: cuda ran out of memory immediately, so I reduced batch size to 20.  However, realized that it may have been a result of killing the previous run, leaving the gpu polluted.  Still, the extra batchnorm is adding some memory, so decided to be safe and leave it.\n",
    "    * Val loss got down to 1.02 on epoch 7.  Memory was 3.88/8.55 GB.\n",
    "* **Note:** There seems to be quite a debate about whether batchnorm should be performed before or after the activation function.  The original paper put it before.  But others seem to find that it is better after.  I agree with the principle of putting it before, since normalizing after the nonlinear activation feels like it is making a false adjustment on only the \"filtered\" outputs.  However, these are the actual outputs of the layer, which is the true input to the next layer.  And it seems that several people have, at least anecdotally, experienced that doing so after the activation provides improved performance.  See \n",
    "    * https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/\n",
    "    * https://forums.fast.ai/t/questions-about-batch-normalization/230/8\n",
    "\n",
    "### Run 15 (11/14/20)\n",
    "* Moved the batchnorm functions to after the relu functions in each of the conv layers 2-5.\n",
    "* Hyperparams:\n",
    "<code>\n",
    "Creating a new, untrained model.\n",
    "CONV_OUT_FEATURES =  98304\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c5): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "  (b5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "  (fc1): Linear(in_features=98304, out_features=3082, bias=True)\n",
    "  (fc2): Linear(in_features=3082, out_features=96, bias=True)\n",
    "  (fc3): Linear(in_features=96, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "Training on GPU\n",
    "LR =  1e-05\n",
    "</code>\n",
    "\n",
    "* Result: Best epoch was #8, with losss of 1.0306, but epoch 10 showed 1.0308, the second-lowest, with a downward trend in training loss, so I thought it might start going down again.  Since I have a checkpoint of that one, I'd like to try continuing from there.  Memory was 4.10/8.57 GB.\n",
    "\n",
    "### Run 16 (11/15/20)\n",
    "* Continuing from run 15, reading from the checkpoint 8, but using a lower learning rate.  This is a test of starting from a checkpoint as much as anything.\n",
    "* New LR = 2e-6\n",
    "\n",
    "* Result:  val loss started out higher than the from run 15 where it left off (1.0493 vs 1.0306 on run 15).  Val loss intermittently found new lows, but the progress was not monotonic.  Even the training loss moves in waves, hitting a minimum, then retreating for 3 or 4 epochs before finding a new low.  Therefore, I think the termination criterion of > 6 epochs with no new low is too tight.\n",
    "* Best epoch was #25, with val loss of 1.0296.  Memory was 3.88/8.55 GB.\n",
    "\n",
    "* It appears that I've hit a wall, unable to find any way for the val loss to go below 1.02 or so.  Need to look for an alternative architecture.\n",
    "\n",
    "### Run 17 (11/15/20)\n",
    "* Looked at visual sampling of the validation & test data sets to ensure that the data there are also reasonable, which they appear to be.\n",
    "* Reduced some of the data manipulations in the transforms, feeling that this may be too much variation given the small sample sizes:\n",
    "    * Scaling was in (0.4, 1.1), changed to (0.8, 1.2)\n",
    "    * affine rotation was 30 deg, changed to 20 deg\n",
    "    * affine translation was 0.2, changed to 0.1\n",
    "    * affine saturation was 0.2, changed to 0.1\n",
    "    * affine hue was 0.2, changed to 0.0, since test images looked mostly brown and train/val samples were lots of bright weird colors.\n",
    "* Increased the validation retry limit from 6 epochs to 10 epochs.\n",
    "* Added logic to run final test.\n",
    "\n",
    "\n",
    "* Result:  \n",
    "    * **The classifier doesn't work at all.**  When running the test code I realized that the model had been trained to always guess class 1 (nevus), and it was right 65% of the time, which is why the losses hit a plateau.  I need to figure out how to build or train a classifier to reach for better results than just guessing the most common class all the time.\n",
    "    * I ran this on my local VM (cpu only).  The epoch times were only slightly longer (\\~16 min) than what I had experienced on the AWS server with a modern GPU (\\~12-13 min).  \n",
    "    * As a further experiment, I rearranged the training code to define the optimizer before moving the model to the gpu, since optimizer needs model params as an input, thinking that interaction may pull it out of the gpu.  Otherwise, I verified that the data, targets and model are all being moved to the gpu before starting both the training loop and the validation loop.  On next run on AWS GPU, I get 12 min/epoch, which isn't a noticeable difference, so the code arrangement didn't matter.  This makes me wonder if I'm doing something else wrong to not take full advantage of the GPU.\n",
    "\n",
    "### Run 18 (11/21/20)\n",
    "\n",
    "* Studied https://stackoverflow.com/questions/41488279/neural-network-always-predicts-the-same-class for some good suggestions on how to trouble-shoot the mis-classification problem.\n",
    "* Removed layer 5 to simplify.\n",
    "* Ran just a batch of melanoma samples (only 51 of training set were kept, and none of the other two classes).\n",
    "    * Result:  in 3 epochs it learned to identify everything as a melanoma, whether it is or not.  It actually appears to learn this in the first epoch (3 batches), but the validation pass in that epoch showed every data sample was classified as a nevus.  I'm not sure how that happens.  Partial output follows:\n",
    "    <code>\n",
    "    Entering training loop.\n",
    "       Train batch: class_count =  [4. 7. 9.] , tgt_count =  [20.  0.  0.]\n",
    "       Train batch: class_count =  [16.  4.  0.] , tgt_count =  [20.  0.  0.]\n",
    "       Train batch: class_count =  [11.  0.  0.] , tgt_count =  [11.  0.  0.]\n",
    "///// Epoch   0: train loss = 1.0184, val loss = 1.0815*, mem 0.00/0.00 GB. Avg 3.1 min/epoch;  2.5 hours rem.\n",
    "      Pred counts:  [  0. 362.   0.] , target counts:  [ 70. 226.  66.]\n",
    "       Train batch: class_count =  [20.  0.  0.] , tgt_count =  [20.  0.  0.]\n",
    "       Train batch: class_count =  [20.  0.  0.] , tgt_count =  [20.  0.  0.]\n",
    "       Train batch: class_count =  [11.  0.  0.] , tgt_count =  [11.  0.  0.]\n",
    "///// Epoch   1: train loss = 0.5527, val loss = 1.0973 , mem 0.00/0.00 GB. Avg 3.0 min/epoch;  2.4 hours rem.\n",
    "      Pred counts:  [313.  49.   0.] , target counts:  [ 70. 226.  66.]\n",
    "    </code>\n",
    "* Ran just a batch of sebhorreic samples (only 27 of them, with nothing in other two classes). \n",
    "    * Result:  similar to those for the melanoma-only run, but it took a little longer to convince the net that this is the true class of choice, maybe because it had fewer data point per epoch?\n",
    "    <code>\n",
    "    Entering training loop.\n",
    "       Train batch: class_count =  [ 0. 10. 10.] , tgt_count =  [ 0.  0. 20.]\n",
    "       Train batch: class_count =  [0. 0. 7.] , tgt_count =  [0. 0. 7.]\n",
    "///// Epoch   0: train loss = 0.9849, val loss = 1.0800*, mem 0.00/0.00 GB. Avg 3.0 min/epoch;  2.5 hours rem.\n",
    "      Pred counts:  [  0. 362.   0.] , target counts:  [ 70. 226.  66.]\n",
    "       Train batch: class_count =  [ 0.  0. 20.] , tgt_count =  [ 0.  0. 20.]\n",
    "       Train batch: class_count =  [0. 0. 7.] , tgt_count =  [0. 0. 7.]\n",
    "///// Epoch   1: train loss = 0.5979, val loss = 1.0837 , mem 0.00/0.00 GB. Avg 3.0 min/epoch;  2.4 hours rem.\n",
    "      Pred counts:  [  0. 362.   0.] , target counts:  [ 70. 226.  66.]\n",
    "       Train batch: class_count =  [ 0.  0. 20.] , tgt_count =  [ 0.  0. 20.]\n",
    "       Train batch: class_count =  [0. 0. 7.] , tgt_count =  [0. 0. 7.]\n",
    "///// Epoch   2: train loss = 0.3321, val loss = 1.0880 , mem 0.00/0.00 GB. Avg 3.0 min/epoch;  2.3 hours rem.\n",
    "      Pred counts:  [  0. 279.  83.] , target counts:  [ 70. 226.  66.]\n",
    "       Train batch: class_count =  [ 0.  0. 20.] , tgt_count =  [ 0.  0. 20.]\n",
    "       Train batch: class_count =  [0. 0. 7.] , tgt_count =  [0. 0. 7.]\n",
    "///// Epoch   3: train loss = 0.2090, val loss = 1.0933 , mem 0.00/0.00 GB. Avg 3.0 min/epoch;  2.3 hours rem.\n",
    "      Pred counts:  [  0.   1. 361.] , target counts:  [ 70. 226.  66.]\n",
    "    </code>\n",
    "    \n",
    "* Ran the same training sample again with some minor changes (notably, leaky_relu now using negative_slope=0.05, not the default 0.01) and more intermediate print stmts.\n",
    "    * It now seems that the training loop quickly converges to the right answer, but the validation loop insists on moving toward class 1 (nevus), like it's running a different model than what came out of the training loop.  [In actuality, it is different, as in eval mode the dropout has no effect and the batchnorm functions differently.] In the val loop, the avg output values are very close to zero, and some are negative. I thought it was going to be guaranteed to be in [0, 1].  Advice article referenced above says don't use softmax if outputs include negatives.\n",
    "    \n",
    "### Run 19 (11/21/20)\n",
    "\n",
    "* Added weights to the cross-entropy criterion to help it solve this unbalanced classifier.\n",
    "* Moved declaration of the optimizer to after moving the model to GPU, per torch docs on the Module.gpu() method.\n",
    "* Verified that I am using the same data augmentation in training & validation. Set it to use in test also.\n",
    "* Changed final FC layer to use relu, rather than leaky_relu, since it must output positive values.\n",
    "* Ran the model with a few melanoma training samples added, so that the training set size was [51, 0, 27].\n",
    "    * All of the training results look reasonable. But the validation loop throws all answers to the seb class (class 2) on each of the first 3 epochs. Output values are getting larger, indicating the model is gaining confidence in its ability to predict, and in each training batch it does seem to.\n",
    "* Added 38 samples of nevus to the data set, so it is now [51, 38. 27].  Reran.\n",
    "    * Similar results as before (preceding bullet).  Each epoch the validation results are wholly thrown to one class (in this case melanoma).  It seems that it will lean to an arbitrary class, then stick with it always.  Maybe that class depends on the initial random weights and/or the direction the first batch leads it.  While each training batch was usually predicted fairly well with just one or two classes represented, in terms of distribution, now that all 3 classes have some data, even the training batches (after the first few) are leaning heavily toward nevus (class 1), regardless of the target distro (while the validation results lean completely to melanoma (class 0)).\n",
    "    * During validation, and even in a couple of the final batches of training in the first epoch (high LR), the output scores are tending toward zero (average winning score = 0.0), which indicates to me that nodes are shutting down, thus not allowing any data to flow through.  Apparently, when the CrossEntropy criterion sees a 3-way tie, it awards the win to the final class in the list, thus I see class 2 (seb) as the one identified for all the validation samples.\n",
    "    * I must conclude that the crux of this problem is that the conv layers are not discriminating on features like they should be.  The FC layers are pretty straightforward things, and, if anything, they have too many neurons.  Therefore, I need to figure out a better structure for the conv layers.\n",
    "    * I do notice that with LR = 0.001 and even 0.0001 I get a lot of class scores of 0 (even for the winning class). But as I refine the LR to 1e-5, while the solution still converges quickly to a single class, at least the winning class has output values > 0, indicating not all nodes are dead.  So perhaps slower training will improve that situation.  Also, may want to consider more dropout.\n",
    "\n",
    "### Run 20 (11/22/20)\n",
    "\n",
    "* Removed all maxpooling and changed the conv filter kernel size from 2 to 4 with stride 2 everywhere.\n",
    "* Moved all training data back into the data set.\n",
    "<code>\n",
    "CONV_OUT_FEATURES =  196608\n",
    "SkinCancerCnn(\n",
    "  (c1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "  (c2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "  (b2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "  (b3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (c4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "  (b4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (fc1): Linear(in_features=196608, out_features=3082, bias=True)\n",
    "  (fc2): Linear(in_features=3082, out_features=96, bias=True)\n",
    "  (fc3): Linear(in_features=96, out_features=3, bias=True)\n",
    "  (dropout): Dropout(p=0.2, inplace=False)\n",
    ")\n",
    "LR =  1e-06\n",
    "</code>\n",
    "\n",
    "* Results:\n",
    "    * When running witn LR = 1e-5, still got all validation outputs pointing to one class after 3 epochs, so changed LR to 1e-6.  Still got the same result.\n",
    "    * Attempted to run the model on AWS just to get timing comparison for the GPU.  Found that it didn't have enough memory to run it, even with batch size = 2.  Tried reducing batch size to 1, but the model blew up because the data loader started returning 3D tensors instead of 4D (no batch index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
