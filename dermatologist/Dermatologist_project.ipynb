{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist Sample Problem\n",
    "This is the optional problem provided in module 3 of the Udacity Deep Learning nanodegree program.  I am building it on my own, after completing the course.\n",
    "\n",
    "I'm trying to decide on a process flow.  Here I'll use the following phases:\n",
    "1. Project setup & acquire data\n",
    "2. Load & prepare the data (includes spot checking & sanity checking for cleanliness & appropriate content)\n",
    "3. Build the model\n",
    "4. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 1:** \n",
    "Data, provided by Udacity, already exists elsewhere in my local directory.  So this is probably a lot shorter than it would normally be.  This data set comes from [2017 ISIC Challenge on Skin Lesion Analysis Towards Melanoma Detection] (https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a).\n",
    "\n",
    "**Phase 2:**\n",
    "* These images are large, and vary in size.  Since I'm looking for lots of details to help improve classification, I don't want to throw away lots of info by unnecessarily croping or downsampling the images. Therefore, I first need to go through the data set and figure out the min/max sizes.  From there I can figure out what is possible.\n",
    "* Consisder using scaling like what is found in https://opensource.com/life/15/2/resize-images-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT_GOAL = 768 #pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-afd78f4654c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "\n",
    "# define training and test data directories\n",
    "#data_dir = '/home/starkj/Udacity/repo/dermatologist/data/' #for local VM\n",
    "data_dir = '/home/ubuntu/models/Udacity/dermatologist/data/' #for AWS server\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "val_dir   = os.path.join(data_dir, \"valid/\")\n",
    "test_dir  = os.path.join(data_dir, 'test/')\n",
    "\n",
    "# classes are folders in each directory with these names\n",
    "classes = ['melanoma', 'nevus', 'seborrheic_keratosis']\n",
    "\n",
    "# load and transform data using ImageFolder\n",
    "\n",
    "# load and transform data using ImageFolder\n",
    "### NOTE:  very important!  Some transforms work on PIL images and some on tensors.  Apply all the\n",
    "###        PIL transforms first, then convert the result to a Tensor, then we can apply further\n",
    "###        transforms if desired.\n",
    "\n",
    "image_size = HEIGHT_GOAL\n",
    "print(\"Using image size: \", image_size)\n",
    "\n",
    "full1 = transforms.Compose([transforms.RandomResizedCrop(image_size, scale=(0.4, 1.1)), \n",
    "                                     transforms.ColorJitter(brightness=0.2, contrast=0.2,\n",
    "                                                            saturation=0.2, hue=0.2),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.RandomVerticalFlip(),\n",
    "                                     transforms.RandomAffine(degrees=30, translate=(0.2, 0.2)),\n",
    "                                     transforms.ToTensor()])\n",
    "no_xform = transforms.ToTensor()\n",
    "crop_only = transforms.Compose([transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "#####\n",
    "##### select the transform set here!\n",
    "#####\n",
    "data_transform = full1\n",
    "\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
    "val_data   = datasets.ImageFolder(val_dir, transform=data_transform)\n",
    "test_data  = datasets.ImageFolder(test_dir, transform=data_transform)\n",
    "\n",
    "# print out some data stats\n",
    "print('Num training images:   ', len(train_data))\n",
    "print('Num validation images: ', len(val_data))\n",
    "print('Num test images:       ', len(test_data))\n",
    "print('training data = ', train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration support function\n",
    "class ImageDatasetStats:\n",
    "    def __init__(self):\n",
    "        self.min_w  = 99999\n",
    "        self.min_w2 = 99999 #second smallest\n",
    "        self.max_w  = 0\n",
    "        self.max_w2 = 0 #second largest\n",
    "        self.min_h  = 99999\n",
    "        self.min_h2 = 99999\n",
    "        self.max_h  = 0\n",
    "        self.max_h2 = 0\n",
    "\n",
    "    # accessors\n",
    "    def get_min_w(self):\n",
    "        return (self.min_w, self.min_w2)\n",
    "\n",
    "    def get_max_w(self):\n",
    "        return (self.max_w, self.max_w2)\n",
    "\n",
    "    def get_min_h(self):\n",
    "        return (self.min_h, self.min_h2)\n",
    "\n",
    "    def get_max_h(self):\n",
    "        return (self.max_h, self.max_h2)\n",
    "    \n",
    "    # set the first and/or second smallest min dimension if new item exceeds\n",
    "    def adjust_mins(self, min1, min2, val):\n",
    "        if val < min1:\n",
    "            min2 = min1\n",
    "            min1 = val\n",
    "        elif val < min2:\n",
    "            min2 = val\n",
    "        return (min1, min2)\n",
    "\n",
    "    # set the first and/or second largest max dimension if new item exceeds\n",
    "    def adjust_maxs(self, max1, max2, val):\n",
    "        if val > max1:\n",
    "            max2 = max1\n",
    "            max1 = val\n",
    "        elif val > max2:\n",
    "            max2 = val\n",
    "        return (max1, max2)\n",
    "\n",
    "    # iterate through an image dataset, finding the first and second largest and smallest values in each dimension\n",
    "    # (width and height)\n",
    "    #    data: ImageFolder\n",
    "    def find2dim_extremes(self, data):\n",
    "        for i in range(len(data)):\n",
    "            im = train_data.__getitem__(i)[0]\n",
    "            h = im.shape[1]\n",
    "            w = im.shape[2]\n",
    "            mins = self.adjust_mins(self.min_w, self.min_w2, w)\n",
    "            maxs = self.adjust_maxs(self.max_w, self.max_w2, w)\n",
    "            self.min_w  = mins[0]\n",
    "            self.min_w2 = mins[1]\n",
    "            self.max_w  = maxs[0]\n",
    "            self.max_w2 = maxs[1]\n",
    "            \n",
    "            mins = self.adjust_mins(self.min_h, self.min_h2, h)\n",
    "            maxs = self.adjust_maxs(self.max_h, self.max_h2, h)\n",
    "            self.min_h  = mins[0]\n",
    "            self.min_h2 = mins[1]\n",
    "            self.max_h  = maxs[0]\n",
    "            self.max_h2 = maxs[1]\n",
    "\n",
    "            if i%10 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "            #print(\"\\nimage shape = \", im.shape)\n",
    "            #print(\"w = \", w, \", h = \", h)\n",
    "            #print(\"width mins & maxes = \", self.min_w, self.min_w2, self.max_w, self.max_w2)\n",
    "            #print(\"height mins & maxs = \", self.min_h, self.min_h2, self.max_h, self.max_h2)\n",
    "   \n",
    "    # iterate through a dataset and count how many images are in each size bin, both height & width\n",
    "    # data: a DataSet of images\n",
    "    def generate_histogram(self, minw, maxw, minh, maxh, num_bins, data):\n",
    "        w_bin_width = int((maxw - minw)/num_bins)\n",
    "        h_bin_width = int((maxh - minh)/num_bins)\n",
    "        \n",
    "        # initialize the arrays that will count the number of items in each size bin\n",
    "        w_count = []\n",
    "        h_count = []\n",
    "        for i in range(num_bins):\n",
    "            w_count.append(0)\n",
    "            h_count.append(0)\n",
    "        \n",
    "        # determine the boundaries of each width bin\n",
    "        w_bin_upper_bound = []\n",
    "        for i in range(num_bins):\n",
    "            w_bin_upper_bound.append(minw + (i+1)*w_bin_width - 1)\n",
    "        w_bin_upper_bound[num_bins-1] = maxw #adjust size of final bin to account for bin width rounding\n",
    "        #print(\"w_bin_upper_bound = \", w_bin_upper_bound)\n",
    "        \n",
    "        # determine the boudnaries of each height bin\n",
    "        h_bin_upper_bound = []\n",
    "        for i in range(num_bins):\n",
    "            h_bin_upper_bound.append(minh + (i+1)*h_bin_width - 1)\n",
    "        h_bin_upper_bound[num_bins-1] = maxh #adjust size of final bin to account for bin width rounding\n",
    "        #print(\"h_bin_upper_bound = \", h_bin_upper_bound)\n",
    "        \n",
    "        # count the number of images that fall into each bin\n",
    "        dsize = len(data)\n",
    "        print(\"Binning images in dataset size \", dsize)\n",
    "        for i in range(dsize):\n",
    "            im = data.__getitem__(i)[0]\n",
    "            h = im.shape[1]\n",
    "            w = im.shape[2]\n",
    "            #print(\"Image \", i, \": h = \", h, \", w = \", w)\n",
    "\n",
    "            for j in range(num_bins):\n",
    "                if w <= w_bin_upper_bound[j]:\n",
    "                    w_count[j] += 1\n",
    "                    break\n",
    "            for j in range(num_bins):\n",
    "                if h <= h_bin_upper_bound[j]:\n",
    "                    h_count[j] += 1\n",
    "                    break\n",
    "        return w_count, h_count, w_bin_upper_bound, h_bin_upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration only\n",
    "'''\n",
    "\n",
    "print(\"train_data shape = \", type(train_data))\n",
    "\n",
    "# Make a loop to look at each image and determine range of sizes\n",
    "stats = ImageDatasetStats()\n",
    "stats.find2dim_extremes(train_data)\n",
    "minw = stats.get_min_w()\n",
    "maxw = stats.get_max_w()\n",
    "minh = stats.get_min_h()\n",
    "maxh = stats.get_max_h()\n",
    "print(\" \")\n",
    "print(\"Width extremes  = \", minw, maxw)\n",
    "print(\"Height extremes = \", minh, maxh)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling variable image sizes\n",
    "\n",
    "Clearly, from above, we have a wide range of image sizes, by almost 10x.  Cropping is not a good answer to this, because many of the images are framed tightly around the artifact of interest, so even a little cropping will be throwing away valuable information.  Other alternatives, which might be combined:\n",
    "\n",
    "1) Downsample the larger images - probably useful for a certain range of downsampling (e.g. up to 4x?) but we stand to lose too much info in the largest images if they are downsampled a lot.  See https://scikit-image.org/docs/dev/auto_examples/transform/plot_rescale.html\n",
    "\n",
    "2) Throw out the smallest and/or largest images as outliers - if there are only a few way out on the tails of the distribution, this could help reduce the magnitude of the problem, but the statistics above only show that there are at least two images at each of the very extremes.\n",
    "\n",
    "3) Build a network that handles variable image sizes - there are some techniques for this, but it involves advanced CNN construction, and adds difficulty to the final FC classification layer.  See:\n",
    "    * https://stats.stackexchange.com/questions/388859/is-it-possible-to-give-variable-sized-images-as-input-to-a-convolutional-neural\n",
    "    * https://www.reddit.com/r/MachineLearning/comments/akbe39/d_best_approach_to_variable_image_sizes_for_image/\n",
    "    * Thesis paper in Downloads folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration only\n",
    "'''\n",
    "stats = ImageDatasetStats()\n",
    "res = stats.generate_histogram(576, 6748, 540, 4499, 11, train_data)\n",
    "\n",
    "print(\"width counts = \", res[0])\n",
    "print(\"width Ubound = \", res[2])\n",
    "print(\"height counts =\", res[1])\n",
    "print(\"height Ubound =\", res[3])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on variable image sizes\n",
    "As the cell above shows, the distribution is far from Gaussian, with ~1/3 of all the images in the lowest size bin.  The upper four size bins combined are about 7% of the images in the training set.  Therefore, for the sake of what is supposed to be a fairly simple exercise here, I'll ignore images with w > 4502 or h > 3052 pixels.  For the remainder, I'll downsample the larger ones and pad the smaller ones.  Since most of the images are in the smallest bin, I don't want to perturb them too much.  The smallest being h=540 and w=576, it probably isn't prudent to pad more than 30%, so set my input image size to 700x748 pixels.  Anything smaller will be padded; anything larger will be downsampled and/or cropped.\n",
    "\n",
    "Problem is, the standard DataLoader won't accept input images that have variable sizes, per below, so need to figure out how to get around that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration only - can't use this as is\n",
    "'''\n",
    "\n",
    "# This code fragment pulled from Udacity DLND module 3 homework.\n",
    "# Note that it only works if all images are identical size (dataiter assumes this)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# define dataloader parameters\n",
    "batch_size = 32\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader) ######### THIS LINE BREAKS - can't handle variable image sizes\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "print(\"images shape = \", images.shape)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading image data of variable sizes\n",
    "\n",
    "* There is an interesting answer for how to do this at https://discuss.pytorch.org/t/torchvision-and-dataloader-different-images-shapes/41026/3. Code from this page is copied below for ease of reading (without scroll bars).\n",
    "* Another, maybe more useful answer, with links to lots of other answers: https://stackoverflow.com/questions/55041080/how-does-pytorch-dataloader-handle-variable-size-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### for image exploration only\n",
    "### I don't think I want to use this, as it has masks that appear to be a substitute for resizing the images,\n",
    "### but it looks like it requires each image to be in a fixed size tensor, which I don't want to do.\n",
    "\n",
    "def default_collate(batch):\n",
    "    \"\"\"\n",
    "    Override `default_collate` https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader\n",
    "\n",
    "    Reference:\n",
    "    def default_collate(batch) at https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader\n",
    "    https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278/3\n",
    "    https://github.com/pytorch/pytorch/issues/1512\n",
    "\n",
    "    We need our own collate function that wraps things up (imge, mask, label).\n",
    "\n",
    "    In this setup,  batch is a list of tuples (the result of calling: img, mask, label = Dataset[i].\n",
    "    The output of this function is four elements:\n",
    "        . data: a pytorch tensor of size (batch_size, c, h, w) of float32 . Each sample is a tensor of shape (c, h_,\n",
    "        w_) that represents a cropped patch from an image (or the entire image) where: c is the depth of the patches (\n",
    "        since they are RGB, so c=3),  h is the height of the patch, and w_ is the its width.\n",
    "        . mask: a list of pytorch tensors of size (batch_size, 1, h, w) full of 1 and 0. The mask of the ENTIRE image (no\n",
    "        cropping is performed). Images does not have the same size, and the same thing goes for the masks. Therefore,\n",
    "        we can't put the masks in one tensor.\n",
    "        . target: a vector (pytorch tensor) of length batch_size of type torch.LongTensor containing the image-level\n",
    "        labels.\n",
    "    :param batch: list of tuples (img, mask, label)\n",
    "    :return: 3 elements: tensor data, list of tensors of masks, tensor of labels.\n",
    "    \"\"\"\n",
    "    data = torch.stack([item[0] for item in batch])\n",
    "    mask = [item[1] for item in batch]  # each element is of size (1, h*, w*). where (h*, w*) changes from mask to another.\n",
    "    target = torch.LongTensor([item[2] for item in batch])  # image labels.\n",
    "\n",
    "    return data, mask, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Defining a custom collator for the DataLoader since its default assumes all raw images are identical\n",
    "# size.  In our case they are not.  This function not only loads different size images, but manipulates\n",
    "# them to be identical size (downsampling and cropping), because the CNN being used in this project\n",
    "# is assuming that all images are identical size.\n",
    "# This function is modified from https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/\n",
    "#\n",
    "# `batch` is a list of tuple where first element is image tensor and\n",
    "# second element is corresponding label\n",
    "#\n",
    "# return is a tuple of (data, target), where data is a tensor of shape [b, c, h, w] and\n",
    "# target is a list of labels\n",
    "\n",
    "AR_GOAL = 1.333333 # represents 768x1024\n",
    "WIDTH_GOAL = HEIGHT_GOAL * AR_GOAL\n",
    "\n",
    "def variable_collate(batch):\n",
    "    data_list = []\n",
    "    for item in batch:\n",
    "        \n",
    "        # get info on the raw image shape\n",
    "        raw_image = item[0]\n",
    "        h = raw_image.shape[1]\n",
    "        w = raw_image.shape[2]\n",
    "        ar = w/h\n",
    "        \n",
    "        # if the aspect ratio is larger than desired, then we scale by the height, otherwise by width\n",
    "        if ar > AR_GOAL:\n",
    "            scale_factor = HEIGHT_GOAL / h\n",
    "        else:\n",
    "            scale_factor = WIDTH_GOAL / w\n",
    "        #print(\"In variable_collate: raw h = \", h, \", w = \", w, \", scale_factor = \", scale_factor)\n",
    "        #print(\"  raw_image = \", raw_image.shape)\n",
    "            \n",
    "        # convert the raw_image tensor into a PIL Image object\n",
    "        image = transforms.ToPILImage()(raw_image.squeeze_(0))\n",
    "        #print(\"  After transform, image = \", image.size)\n",
    "        \n",
    "        # scale it, preserving aspect ratio\n",
    "        resized = image.resize((int(w*scale_factor), int(h*scale_factor)))\n",
    "        width = resized.width\n",
    "        height = resized.height\n",
    "        #print(\"  resized w = \", width, \", h = \", height)\n",
    "            \n",
    "        # now we have an image that doesn't necessarily match our goal aspect ratio, but is at least\n",
    "        # as large as our goal dimensions in both directions; need to center-crop it - \n",
    "        # DON'T try to make it square!\n",
    "        if width > WIDTH_GOAL:\n",
    "            left = (width - WIDTH_GOAL) // 2\n",
    "        else:\n",
    "            left = 0\n",
    "        right = left + WIDTH_GOAL #don't subtract 1; this is the next pixel beyond the edge of the image\n",
    "        \n",
    "        if height > HEIGHT_GOAL:\n",
    "            upper = (height - HEIGHT_GOAL) // 2\n",
    "        else:\n",
    "            upper = 0\n",
    "        lower = upper + HEIGHT_GOAL #don't subtract 1\n",
    "        #print(\"  cropping to left = \", left, \", right = \", right, \", upper = \", upper, \", lower = \", lower)\n",
    "        \n",
    "        cropped_image = resized.crop((left, upper, right, lower))\n",
    "        \n",
    "        # convert the properly sized image back to a tensor\n",
    "        cropped_tensor = transforms.ToTensor()(cropped_image).unsqueeze_(0)\n",
    "        data_list.append(cropped_tensor)\n",
    "        #print(\"  cropped_tensor = \", cropped_tensor.shape)\n",
    "        \n",
    "\n",
    "    # assemble all of the images into the data tensor for output\n",
    "    data = torch.stack(data_list).squeeze()\n",
    "    #print(\"  returning data = \", data.shape)\n",
    "    \n",
    "    # gather the target labels for each image\n",
    "    target = [item[1] for item in batch]\n",
    "    target = torch.LongTensor(target)\n",
    "    \n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader parameters\n",
    "batch_size = 32\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, collate_fn=variable_collate, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_data, collate_fn=variable_collate, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, collate_fn=variable_collate, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### For image exploration only\n",
    "'''\n",
    "# This code fragment pulled from Udacity DLND module 3 homework.\n",
    "# Note that it only works if all images are identical size (dataiter assumes this)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# obtain one batch of training images - these need to be in a tensor of shape [b, c, w, h]\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"images type from dataiter = \", type(images))\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "print(\"images shape = \", images.shape)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 8)) #numbers indicate horiz & vert spacing between images\n",
    "for idx in np.arange(32):\n",
    "    ax = fig.add_subplot(4, 32/4, idx+1, xticks=[], yticks=[]) #num rows, num images/row\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These images look pretty good!  Sizes of the objects of interest aren't as uniform as I'd like, but for the purposes of this exercise, I think close enough.  To get better, it's probably worth doing some pre-processing with some sort of edge/feature detection to define a bounding box in the raw image, then crop around that before doing the scaling I've done here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model\n",
    "\n",
    "I would like to use transfer learning with one of the big CNN's, but I need one that will use my chosen image size, or something close to it.\n",
    "* VGG uses 224x224\n",
    "* ResNet50 - not clear, but it looks like it can handle different sizes, up to 640 on short side.\n",
    "* GoogLeNet 50 appears to take 224x224 also.\n",
    "\n",
    "I'm starting to think all of these models are built for 224 square as a de facto standard for benchmarking.  Therefore, I choose to write my own model from scratch.  There is no doubt it will perform much worse than these world class models, but it will have one advantage in being able to use more input data (more pixels in the input images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkinCancerCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkinCancerCnn, self).__init__()\n",
    "        \n",
    "        self.CONV_OUT_FEATURES = int(128 * (768/2/2/2/2/2) * (1024/2/2/2/2/2))\n",
    "        print(\"CONV_OUT_FEATURES = \", self.CONV_OUT_FEATURES)\n",
    "        \n",
    "        #input images are 768x1024 pixels, full coloer (3 channels)\n",
    "        self.c1 = nn.Conv2d(3, 8, 2, padding=1) #stride = 1\n",
    "        self.c2 = nn.Conv2d(8, 16, 2, padding=1)\n",
    "        self.c3 = nn.Conv2d(16, 32, 2, padding=1)\n",
    "        self.c4 = nn.Conv2d(32, 64, 2, padding=1)\n",
    "        self.c5 = nn.Conv2d(64, 128, 2, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        #linear classifier\n",
    "        self.fc1 = nn.Linear(self.CONV_OUT_FEATURES, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.c1(x)) #output 8 x 768 x 1024\n",
    "        x = self.pool(x)       #output 8 x 384 x 512\n",
    "        \n",
    "        x = F.relu(self.c2(x)) #ouptut 16 x 384 x 512\n",
    "        x = self.pool(x)       #output 16 x 192 x 256\n",
    "        \n",
    "        x = F.relu(self.c3(x)) #output 32 x 192 x 256\n",
    "        x = self.pool(x)       #output 32 x 96 x 128\n",
    "        \n",
    "        x = F.relu(self.c4(x)) #output 64 x 96 x 128\n",
    "        x = self.pool(x)       #output 64 x 48 x 64\n",
    "        \n",
    "        x = F.relu(self.c5(x)) #output 128 x 48 x 64\n",
    "        x = self.pool(x)       #output 128 x 24 x 32\n",
    "        \n",
    "        conv_out = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        #print(\"forward: after conv, x.shape = \", x.shape, \"conv_out = \", conv_out)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1) #flatten the convolved image\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x)) # softmax will be applied during training (as part of loss function), not here\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "model = SkinCancerCnn()\n",
    "print(model)\n",
    "if train_on_gpu:\n",
    "    model = model.cuda()\n",
    "    \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "num_epochs = 30\n",
    "min_val_loss = 999999.0\n",
    "min_val_epoch = 0\n",
    "val_retry_limit = 5\n",
    "\n",
    "print(\"Entering training loop.\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() #put it into training mode\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        if train_on_gpu:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*data.size(0) #CrossEntropy already divides by num items in batch\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    # validate the training on this epoch\n",
    "    model.eval() #put into evaluation mode\n",
    "    val_loss = 0.0\n",
    "    for data, target in val_loader:\n",
    "        if train_on_gpu:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        val_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    # determine if training is complete, based on validation loss hitting a minimum recently\n",
    "    flag = ' ' \n",
    "    if val_loss <= min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        min_val_epoch = epoch\n",
    "        flag = '*'\n",
    "        \n",
    "    # show epoch results\n",
    "    print(\"///// Epoch {}: training loss = {:.6f}   validation loss = {:.6f}{}\".format(\n",
    "        epoch, train_loss, val_loss, flag))\n",
    "    \n",
    "    if epoch > (min_val_epoch + val_retry_limit):\n",
    "        print(\"      Training terminated due to increasing validation loss.\")\n",
    "        break\n",
    "    \n",
    "print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AWS\n",
    "Ran the above code for 2 epochs on the local computer to determine that the model is functional and seems to be trainable (loss decreased).\n",
    "\n",
    "Now it's time to move it to an AWS instance in order to train.  I discovered my existing instance does not have any storage attached (I had been using a separate EBS volume, and deleted it after class).  Therefore, choosing a new instance:  <code>g2.2xlarge</code>, which has 1 GPU and 2 cores of CPU (2 threads each) and 16 GB of memory.\n",
    "\n",
    "System volume is /dev/sda1.  Working data storage is on /dev/sdb (60 GB).  Instance ID is i-0be255fe5cb1becf1.  Username is default \"ubuntu\".  Changed pwd to H1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Ubuntu VM disk size problem\n",
    "\n",
    "Diversion for a few days...\n",
    "\n",
    "Got into trouble by filling up my hard drive.  Took a long time to move data off and back it up, then figure out how to extend the drive space in VMWare.  \n",
    "* First, shut down the VM, then in Settings select Extend drive space.  This enlarges the physical drive only.  It doesn't extend the partition or the file system in the partition.\n",
    "* Next use gparted to enlarge the physical partition (I did not have a logical volume, so LVM was not an option).  Decent instructions here to get started, then just follow intuitin and the UI:  https://unix.stackexchange.com/questions/196512/how-to-extend-filesystem-partition-on-ubuntu-vm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transfer\n",
    "\n",
    "Now that VM drive space problem is solved, I re-downloaded all of the data from the original Udacity problem and moved it into my Github repo under dermatologist/data.  This took quite a while, as each push has to complete within a certain timeout period (not clear to me what causes that, but it appears it's my host screen saver time that essentially shuts down VM activity).\n",
    "\n",
    "Attempted to pull the entire dermatologist branch down to my AWS server, but ran out of drive space there. Cleaned it up and pulled all the data.  Now ready to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "### Error using GPU\n",
    "Upon first run, I got the following error:\n",
    "\n",
    "<code>\n",
    "/home/ubuntu/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:102: UserWarning: \n",
    "    Found GPU0 GRID K520 which is of cuda capability 3.0.\n",
    "    PyTorch no longer supports this GPU because it is too old.\n",
    "    The minimum cuda capability that we support is 3.5.\n",
    "</code>\n",
    "\n",
    "It would appear that I've purchased an inappropriate server for my work!  I currently have Pytorch 1.6.0 installed.  Forums say to drop back to 0.3.0 to get support for cuda capability 3.0.  That's a long way back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "device_id = torch.cuda.current_device()\n",
    "gpu_properties = torch.cuda.get_device_properties(device_id)\n",
    "print(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n",
    "      \"%.1fGb total memory.\\n\" % \n",
    "      (torch.cuda.device_count(),\n",
    "      device_id,\n",
    "      gpu_properties.name,\n",
    "      gpu_properties.major,\n",
    "      gpu_properties.minor,\n",
    "      gpu_properties.total_memory / 1e9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9/27/20:\n",
    "\n",
    "AWS says that\n",
    "\n",
    "* p3.2xlarge instance has one V100 GPU, which has compute capability of 7.0, plus 61 GB memory for $3.06/hr.\n",
    "\n",
    "* p2.xlarge instance has one K80 GPU, which provides compute capability of 3.7, plus 61 GB memory for $0.9/hr.\n",
    "\n",
    "* The g3 and g4 instances are intended for graphics-intense apps, and don't seem to provide an advantage here.\n",
    "\n",
    "I probably want to use the Deep Learning AMI (DLAMI), described here: https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html\n",
    "\n",
    "I want to use the Ubuntu 18.04 Conda variant.  I don't believe this is what I have on the 664 instance from class, so I'll create a new one, just to be sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10/24/20:\n",
    "\n",
    "Created a new EC2 instance, **ID = i-03ffe00145cb0b929**\n",
    "* Built from DLAMI Ubuntu 18.04 v35.0\n",
    "* Reuses security group created on 9/10, ID = sg-0b8d086b2fa6881a7 (launch-wizard-4), which is wide open on 5 ports.\n",
    "* Storage is EBS volume ID = vol-043c36a11deb7c7cd, which is 130 GB of gp2, and marked as delete on termination.\n",
    "    * The AMI takes up 74 GB\n",
    "    * The Udacity repo currently takes up 21 GB\n",
    "    * Total disk usage is 90 GB before I even start working.\n",
    "\n",
    "* Found that torch & torchvision were not installed, despite the conda list being very long (and I thought the advertised DL AMI included pytorch).\n",
    "    * Resolved that by using command <code>conda install -c pytorch pytorch torchvision</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
