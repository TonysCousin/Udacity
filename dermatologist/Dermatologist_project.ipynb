{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dermatologist Sample Problem\n",
    "This is the optional problem provided in module 3 of the Udacity Deep Learning nanodegree program.  I am building it on my own, after completing the course.\n",
    "\n",
    "I'm trying to decide on a process flow.  Here I'll use the following phases:\n",
    "1. Project setup & acquire data\n",
    "2. Load & prepare the data (includes spot checking & sanity checking for cleanliness & appropriate content)\n",
    "3. Build the model\n",
    "4. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phase 1:** \n",
    "Data, provided by Udacity, already exists elsewhere in my local directory.  So this is probably a lot shorter than it would normally be.  This data set comes from [2017 ISIC Challenge on Skin Lesion Analysis Towards Melanoma Detection] (https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a).\n",
    "\n",
    "**Phase 2:**\n",
    "* These images are large, and vary in size.  Since I'm looking for lots of details to help improve classification, I don't want to throw away lots of info by unnecessarily croping or downsampling the images. Therefore, I first need to go through the data set and figure out the min/max sizes.  From there I can figure out what is possible.\n",
    "* Consisder using scaling like what is found in https://opensource.com/life/15/2/resize-images-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT_GOAL = 768 #pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n",
      "Using image size:  768\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/models/Udacity/Module3/dermatologist-ai/data/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c17afe4136d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mdata_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_xform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mval_data\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mtest_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    203\u001b[0m     def __init__(self, root, transform=None, target_transform=None,\n\u001b[1;32m    204\u001b[0m                  loader=default_loader, is_valid_file=None):\n\u001b[0;32m--> 205\u001b[0;31m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n\u001b[0m\u001b[1;32m    206\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     92\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     93\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \"\"\"\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/models/Udacity/Module3/dermatologist-ai/data/train/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    \n",
    "\n",
    "# define training and test data directories\n",
    "#data_dir = '/home/starkj/Udacity/Module3/dermatologist-ai/data/'\n",
    "data_dir = '/home/ubuntu/models/Udacity/Module3/dermatologist-ai/data/'\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "val_dir   = os.path.join(data_dir, \"valid/\")\n",
    "test_dir  = os.path.join(data_dir, 'test/')\n",
    "\n",
    "# classes are folders in each directory with these names\n",
    "classes = ['melanoma', 'nevus', 'seborrheic_keratosis']\n",
    "\n",
    "# load and transform data using ImageFolder\n",
    "\n",
    "# load and transform data using ImageFolder\n",
    "### NOTE:  very important!  Some transforms work on PIL images and some on tensors.  Apply all the\n",
    "###        PIL transforms first, then convert the result to a Tensor, then we can apply further\n",
    "###        transforms if desired.\n",
    "\n",
    "# VGG-16 Takes 224x224 images as input, so we resize all of them\n",
    "image_size = HEIGHT_GOAL\n",
    "print(\"Using image size: \", image_size)\n",
    "\n",
    "full1 = transforms.Compose([transforms.RandomResizedCrop(image_size), \n",
    "                                     transforms.ColorJitter(brightness=0.2, contrast=0.2,\n",
    "                                                            saturation=0.2, hue=0.2),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.RandomVerticalFlip(),\n",
    "                                     transforms.RandomAffine(degrees=30, translate=(0.2, 0.2),\n",
    "                                                             scale=(0.2, 0.2)),\n",
    "                                     transforms.ToTensor()])\n",
    "no_xform = transforms.ToTensor()\n",
    "crop_only = transforms.Compose([transforms.CenterCrop(image_size),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "data_transform = no_xform\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
    "val_data   = datasets.ImageFolder(val_dir, transform=data_transform)\n",
    "test_data  = datasets.ImageFolder(test_dir, transform=data_transform)\n",
    "\n",
    "# print out some data stats\n",
    "print('Num training images:   ', len(train_data))\n",
    "print('Num validation images: ', len(val_data))\n",
    "print('Num test images:       ', len(test_data))\n",
    "print('training data = ', train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration support function\n",
    "class ImageDatasetStats:\n",
    "    def __init__(self):\n",
    "        self.min_w  = 99999\n",
    "        self.min_w2 = 99999 #second smallest\n",
    "        self.max_w  = 0\n",
    "        self.max_w2 = 0 #second largest\n",
    "        self.min_h  = 99999\n",
    "        self.min_h2 = 99999\n",
    "        self.max_h  = 0\n",
    "        self.max_h2 = 0\n",
    "\n",
    "    # accessors\n",
    "    def get_min_w(self):\n",
    "        return (self.min_w, self.min_w2)\n",
    "\n",
    "    def get_max_w(self):\n",
    "        return (self.max_w, self.max_w2)\n",
    "\n",
    "    def get_min_h(self):\n",
    "        return (self.min_h, self.min_h2)\n",
    "\n",
    "    def get_max_h(self):\n",
    "        return (self.max_h, self.max_h2)\n",
    "    \n",
    "    # set the first and/or second smallest min dimension if new item exceeds\n",
    "    def adjust_mins(self, min1, min2, val):\n",
    "        if val < min1:\n",
    "            min2 = min1\n",
    "            min1 = val\n",
    "        elif val < min2:\n",
    "            min2 = val\n",
    "        return (min1, min2)\n",
    "\n",
    "    # set the first and/or second largest max dimension if new item exceeds\n",
    "    def adjust_maxs(self, max1, max2, val):\n",
    "        if val > max1:\n",
    "            max2 = max1\n",
    "            max1 = val\n",
    "        elif val > max2:\n",
    "            max2 = val\n",
    "        return (max1, max2)\n",
    "\n",
    "    # iterate through an image dataset, finding the first and second largest and smallest values in each dimension\n",
    "    # (width and height)\n",
    "    #    data: ImageFolder\n",
    "    def find2dim_extremes(self, data):\n",
    "        for i in range(len(data)):\n",
    "            im = train_data.__getitem__(i)[0]\n",
    "            h = im.shape[1]\n",
    "            w = im.shape[2]\n",
    "            mins = self.adjust_mins(self.min_w, self.min_w2, w)\n",
    "            maxs = self.adjust_maxs(self.max_w, self.max_w2, w)\n",
    "            self.min_w  = mins[0]\n",
    "            self.min_w2 = mins[1]\n",
    "            self.max_w  = maxs[0]\n",
    "            self.max_w2 = maxs[1]\n",
    "            \n",
    "            mins = self.adjust_mins(self.min_h, self.min_h2, h)\n",
    "            maxs = self.adjust_maxs(self.max_h, self.max_h2, h)\n",
    "            self.min_h  = mins[0]\n",
    "            self.min_h2 = mins[1]\n",
    "            self.max_h  = maxs[0]\n",
    "            self.max_h2 = maxs[1]\n",
    "\n",
    "            if i%10 == 0:\n",
    "                print(\".\", end = \"\")\n",
    "            '''\n",
    "            print(\"\\nimage shape = \", im.shape)\n",
    "            print(\"w = \", w, \", h = \", h)\n",
    "            print(\"width mins & maxes = \", self.min_w, self.min_w2, self.max_w, self.max_w2)\n",
    "            print(\"height mins & maxs = \", self.min_h, self.min_h2, self.max_h, self.max_h2)\n",
    "            '''\n",
    "   \n",
    "    # iterate through a dataset and count how many images are in each size bin, both height & width\n",
    "    # data: a DataSet of images\n",
    "    def generate_histogram(self, minw, maxw, minh, maxh, num_bins, data):\n",
    "        w_bin_width = int((maxw - minw)/num_bins)\n",
    "        h_bin_width = int((maxh - minh)/num_bins)\n",
    "        \n",
    "        # initialize the arrays that will count the number of items in each size bin\n",
    "        w_count = []\n",
    "        h_count = []\n",
    "        for i in range(num_bins):\n",
    "            w_count.append(0)\n",
    "            h_count.append(0)\n",
    "        \n",
    "        # determine the boundaries of each width bin\n",
    "        w_bin_upper_bound = []\n",
    "        for i in range(num_bins):\n",
    "            w_bin_upper_bound.append(minw + (i+1)*w_bin_width - 1)\n",
    "        w_bin_upper_bound[num_bins-1] = maxw #adjust size of final bin to account for bin width rounding\n",
    "        #print(\"w_bin_upper_bound = \", w_bin_upper_bound)\n",
    "        \n",
    "        # determine the boudnaries of each height bin\n",
    "        h_bin_upper_bound = []\n",
    "        for i in range(num_bins):\n",
    "            h_bin_upper_bound.append(minh + (i+1)*h_bin_width - 1)\n",
    "        h_bin_upper_bound[num_bins-1] = maxh #adjust size of final bin to account for bin width rounding\n",
    "        #print(\"h_bin_upper_bound = \", h_bin_upper_bound)\n",
    "        \n",
    "        # count the number of images that fall into each bin\n",
    "        dsize = len(data)\n",
    "        print(\"Binning images in dataset size \", dsize)\n",
    "        for i in range(dsize):\n",
    "            im = data.__getitem__(i)[0]\n",
    "            h = im.shape[1]\n",
    "            w = im.shape[2]\n",
    "            #print(\"Image \", i, \": h = \", h, \", w = \", w)\n",
    "\n",
    "            for j in range(num_bins):\n",
    "                if w <= w_bin_upper_bound[j]:\n",
    "                    w_count[j] += 1\n",
    "                    break\n",
    "            for j in range(num_bins):\n",
    "                if h <= h_bin_upper_bound[j]:\n",
    "                    h_count[j] += 1\n",
    "                    break\n",
    "        return w_count, h_count, w_bin_upper_bound, h_bin_upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration only\n",
    "'''\n",
    "\n",
    "print(\"train_data shape = \", type(train_data))\n",
    "\n",
    "# Make a loop to look at each image and determine range of sizes\n",
    "stats = ImageDatasetStats()\n",
    "stats.find2dim_extremes(train_data)\n",
    "minw = stats.get_min_w()\n",
    "maxw = stats.get_max_w()\n",
    "minh = stats.get_min_h()\n",
    "maxh = stats.get_max_h()\n",
    "print(\" \")\n",
    "print(\"Width extremes  = \", minw, maxw)\n",
    "print(\"Height extremes = \", minh, maxh)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling variable image sizes\n",
    "\n",
    "Clearly, from above, we have a wide range of image sizes, by almost 10x.  Cropping is not a good answer to this, because many of the images are framed tightly around the artifact of interest, so even a little cropping will be throwing away valuable information.  Other alternatives, which might be combined:\n",
    "\n",
    "1) Downsample the larger images - probably useful for a certain range of downsampling (e.g. up to 4x?) but we stand to lose too much info in the largest images if they are downsampled a lot.  See https://scikit-image.org/docs/dev/auto_examples/transform/plot_rescale.html\n",
    "\n",
    "2) Throw out the smallest and/or largest images as outliers - if there are only a few way out on the tails of the distribution, this could help reduce the magnitude of the problem, but the statistics above only show that there are at least two images at each of the very extremes.\n",
    "\n",
    "3) Build a network that handles variable image sizes - there are some techniques for this, but it involves advanced CNN construction, and adds difficulty to the final FC classification layer.  See:\n",
    "    * https://stats.stackexchange.com/questions/388859/is-it-possible-to-give-variable-sized-images-as-input-to-a-convolutional-neural\n",
    "    * https://www.reddit.com/r/MachineLearning/comments/akbe39/d_best_approach_to_variable_image_sizes_for_image/\n",
    "    * Thesis paper in Downloads folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration only\n",
    "'''\n",
    "stats = ImageDatasetStats()\n",
    "res = stats.generate_histogram(576, 6748, 540, 4499, 11, train_data)\n",
    "\n",
    "print(\"width counts = \", res[0])\n",
    "print(\"width Ubound = \", res[2])\n",
    "print(\"height counts =\", res[1])\n",
    "print(\"height Ubound =\", res[3])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on variable image sizes\n",
    "As the cell above shows, the distribution is far from Gaussian, with ~1/3 of all the images in the lowest size bin.  The upper four size bins combined are about 7% of the images in the training set.  Therefore, for the sake of what is supposed to be a fairly simple exercise here, I'll ignore images with w > 4502 or h > 3052 pixels.  For the remainder, I'll downsample the larger ones and pad the smaller ones.  Since most of the images are in the smallest bin, I don't want to perturb them too much.  The smallest being h=540 and w=576, it probably isn't prudent to pad more than 30%, so set my input image size to 700x748 pixels.  Anything smaller will be padded; anything larger will be downsampled and/or cropped.\n",
    "\n",
    "Problem is, the standard DataLoader won't accept input images that have variable sizes, per below, so need to figure out how to get around that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### data exploration only - can't use this as is\n",
    "'''\n",
    "\n",
    "# This code fragment pulled from Udacity DLND module 3 homework.\n",
    "# Note that it only works if all images are identical size (dataiter assumes this)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# define dataloader parameters\n",
    "batch_size = 32\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader) ######### THIS LINE BREAKS - can't handle variable image sizes\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "print(\"images shape = \", images.shape)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading image data of variable sizes\n",
    "\n",
    "* There is an interesting answer for how to do this at https://discuss.pytorch.org/t/torchvision-and-dataloader-different-images-shapes/41026/3. Code from this page is copied below for ease of reading (without scroll bars).\n",
    "* Another, maybe more useful answer, with links to lots of other answers: https://stackoverflow.com/questions/55041080/how-does-pytorch-dataloader-handle-variable-size-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### for image exploration only\n",
    "### I don't think I want to use this, as it has masks that appear to be a substitute for resizing the images,\n",
    "### but it looks like it requires each image to be in a fixed size tensor, which I don't want to do.\n",
    "\n",
    "def default_collate(batch):\n",
    "    \"\"\"\n",
    "    Override `default_collate` https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader\n",
    "\n",
    "    Reference:\n",
    "    def default_collate(batch) at https://pytorch.org/docs/stable/_modules/torch/utils/data/dataloader.html#DataLoader\n",
    "    https://discuss.pytorch.org/t/how-to-create-a-dataloader-with-variable-size-input/8278/3\n",
    "    https://github.com/pytorch/pytorch/issues/1512\n",
    "\n",
    "    We need our own collate function that wraps things up (imge, mask, label).\n",
    "\n",
    "    In this setup,  batch is a list of tuples (the result of calling: img, mask, label = Dataset[i].\n",
    "    The output of this function is four elements:\n",
    "        . data: a pytorch tensor of size (batch_size, c, h, w) of float32 . Each sample is a tensor of shape (c, h_,\n",
    "        w_) that represents a cropped patch from an image (or the entire image) where: c is the depth of the patches (\n",
    "        since they are RGB, so c=3),  h is the height of the patch, and w_ is the its width.\n",
    "        . mask: a list of pytorch tensors of size (batch_size, 1, h, w) full of 1 and 0. The mask of the ENTIRE image (no\n",
    "        cropping is performed). Images does not have the same size, and the same thing goes for the masks. Therefore,\n",
    "        we can't put the masks in one tensor.\n",
    "        . target: a vector (pytorch tensor) of length batch_size of type torch.LongTensor containing the image-level\n",
    "        labels.\n",
    "    :param batch: list of tuples (img, mask, label)\n",
    "    :return: 3 elements: tensor data, list of tensors of masks, tensor of labels.\n",
    "    \"\"\"\n",
    "    data = torch.stack([item[0] for item in batch])\n",
    "    mask = [item[1] for item in batch]  # each element is of size (1, h*, w*). where (h*, w*) changes from mask to another.\n",
    "    target = torch.LongTensor([item[2] for item in batch])  # image labels.\n",
    "\n",
    "    return data, mask, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Defining a custom collator for the DataLoader since its default assumes all raw images are identical\n",
    "# size.  In our case they are not.  This function not only loads different size images, but manipulates\n",
    "# them to be identical size (downsampling and cropping), because the CNN being used in this project\n",
    "# is assuming that all images are identical size.\n",
    "# This function is modified from https://jdhao.github.io/2017/10/23/pytorch-load-data-and-make-batch/\n",
    "#\n",
    "# `batch` is a list of tuple where first element is image tensor and\n",
    "# second element is corresponding label\n",
    "#\n",
    "# return is a tuple of (data, target), where data is a tensor of shape [b, c, h, w] and\n",
    "# target is a list of labels\n",
    "\n",
    "AR_GOAL = 1.333333 # represents 768x1024\n",
    "WIDTH_GOAL = HEIGHT_GOAL * AR_GOAL\n",
    "\n",
    "def variable_collate(batch):\n",
    "    data_list = []\n",
    "    for item in batch:\n",
    "        \n",
    "        # get info on the raw image shape\n",
    "        raw_image = item[0]\n",
    "        h = raw_image.shape[1]\n",
    "        w = raw_image.shape[2]\n",
    "        ar = w/h\n",
    "        \n",
    "        # if the aspect ratio is larger than desired, then we scale by the height, otherwise by width\n",
    "        if ar > AR_GOAL:\n",
    "            scale_factor = HEIGHT_GOAL / h\n",
    "        else:\n",
    "            scale_factor = WIDTH_GOAL / w\n",
    "        #print(\"In variable_collate: raw h = \", h, \", w = \", w, \", scale_factor = \", scale_factor)\n",
    "        #print(\"  raw_image = \", raw_image.shape)\n",
    "            \n",
    "        # convert the raw_image tensor into a PIL Image object\n",
    "        image = transforms.ToPILImage()(raw_image.squeeze_(0))\n",
    "        #print(\"  After transform, image = \", image.size)\n",
    "        \n",
    "        # scale it, preserving aspect ratio\n",
    "        resized = image.resize((int(w*scale_factor), int(h*scale_factor)))\n",
    "        width = resized.width\n",
    "        height = resized.height\n",
    "        #print(\"  resized w = \", width, \", h = \", height)\n",
    "            \n",
    "        # now we have an image that doesn't necessarily match our goal aspect ratio, but is at least\n",
    "        # as large as our goal dimensions in both directions; need to center-crop it - \n",
    "        # DON'T try to make it square!\n",
    "        if width > WIDTH_GOAL:\n",
    "            left = (width - WIDTH_GOAL) // 2\n",
    "        else:\n",
    "            left = 0\n",
    "        right = left + WIDTH_GOAL #don't subtract 1; this is the next pixel beyond the edge of the image\n",
    "        \n",
    "        if height > HEIGHT_GOAL:\n",
    "            upper = (height - HEIGHT_GOAL) // 2\n",
    "        else:\n",
    "            upper = 0\n",
    "        lower = upper + HEIGHT_GOAL #don't subtract 1\n",
    "        #print(\"  cropping to left = \", left, \", right = \", right, \", upper = \", upper, \", lower = \", lower)\n",
    "        \n",
    "        cropped_image = resized.crop((left, upper, right, lower))\n",
    "        \n",
    "        # convert the properly sized image back to a tensor\n",
    "        cropped_tensor = transforms.ToTensor()(cropped_image).unsqueeze_(0)\n",
    "        data_list.append(cropped_tensor)\n",
    "        #print(\"  cropped_tensor = \", cropped_tensor.shape)\n",
    "        \n",
    "\n",
    "    # assemble all of the images into the data tensor for output\n",
    "    data = torch.stack(data_list).squeeze()\n",
    "    #print(\"  returning data = \", data.shape)\n",
    "    \n",
    "    # gather the target labels for each image\n",
    "    target = [item[1] for item in batch]\n",
    "    target = torch.LongTensor(target)\n",
    "    \n",
    "    return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader parameters\n",
    "batch_size = 32\n",
    "num_workers=0\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, collate_fn=variable_collate, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader   = torch.utils.data.DataLoader(val_data, collate_fn=variable_collate, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, collate_fn=variable_collate, batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code fragment pulled from Udacity DLND module 3 homework.\n",
    "# Note that it only works if all images are identical size (dataiter assumes this)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# obtain one batch of training images - these need to be in a tensor of shape [b, c, w, h]\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "print(\"images type from dataiter = \", type(images))\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "print(\"images shape = \", images.shape)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 8)) #numbers indicate horiz & vert spacing between images\n",
    "for idx in np.arange(32):\n",
    "    ax = fig.add_subplot(4, 32/4, idx+1, xticks=[], yticks=[]) #num rows, num images/row\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
    "    ax.set_title(classes[labels[idx]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these images look pretty good!  Sizes of the objects of interest aren't as uniform as I'd like, but for the purposes of this exercise, I think close enough.  To get better, it's probably worth doing some pre-processing with some sort of edge/feature detection to define a bounding box in the raw image, then crop around that before doing the scaling I've done here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Model\n",
    "\n",
    "I would like to use transfer learning with one of the big CNN's, but I need one that will use my chosen image size, or something close to it.\n",
    "* VGG uses 224x224\n",
    "* ResNet50 - not clear, but it looks like it can handle different sizes, up to 640 on short side.\n",
    "* GoogLeNet 50 appears to take 224x224 also.\n",
    "\n",
    "I'm starting to think all of these models are built for 224 square as a de facto standard for benchmarking.  Therefore, I choose to write my own model from scratch.  There is no doubt it will perform much worse than these world class models, but it will have one advantage in being able to use more input data (more pixels in the input images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SkinCancerCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkinCancerCnn, self).__init__()\n",
    "        \n",
    "        self.CONV_OUT_FEATURES = int(128 * (768/2/2/2/2/2) * (1024/2/2/2/2/2))\n",
    "        print(\"CONV_OUT_FEATURES = \", self.CONV_OUT_FEATURES)\n",
    "        \n",
    "        #input images are 768x1024 pixels, full coloer (3 channels)\n",
    "        self.c1 = nn.Conv2d(3, 8, 2, padding=1) #stride = 1\n",
    "        self.c2 = nn.Conv2d(8, 16, 2, padding=1)\n",
    "        self.c3 = nn.Conv2d(16, 32, 2, padding=1)\n",
    "        self.c4 = nn.Conv2d(32, 64, 2, padding=1)\n",
    "        self.c5 = nn.Conv2d(64, 128, 2, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        #linear classifier\n",
    "        self.fc1 = nn.Linear(self.CONV_OUT_FEATURES, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.c1(x)) #output 8 x 768 x 1024\n",
    "        x = self.pool(x)       #output 8 x 384 x 512\n",
    "        \n",
    "        x = F.relu(self.c2(x)) #ouptut 16 x 384 x 512\n",
    "        x = self.pool(x)       #output 16 x 192 x 256\n",
    "        \n",
    "        x = F.relu(self.c3(x)) #output 32 x 192 x 256\n",
    "        x = self.pool(x)       #output 32 x 96 x 128\n",
    "        \n",
    "        x = F.relu(self.c4(x)) #output 64 x 96 x 128\n",
    "        x = self.pool(x)       #output 64 x 48 x 64\n",
    "        \n",
    "        x = F.relu(self.c5(x)) #output 128 x 48 x 64\n",
    "        x = self.pool(x)       #output 128 x 24 x 32\n",
    "        \n",
    "        conv_out = x.shape[1] * x.shape[2] * x.shape[3]\n",
    "        #print(\"forward: after conv, x.shape = \", x.shape, \"conv_out = \", conv_out)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1) #flatten the convolved image\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x)) # softmax will be applied during training (as part of loss function), not here\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SkinCancerCnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-52c9b881da6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkinCancerCnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SkinCancerCnn' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model = SkinCancerCnn()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "print(\"Entering training loop.\")\n",
    "model.train() #put it into training mode\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    print(\"///// Epoch {}: loss = {:.6f}\".format(epoch, train_loss))\n",
    "    \n",
    "print(\"Done training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AWS\n",
    "Ran the above code for 2 epochs on the local computer to determine that the model is functional and seems to be trainable (loss decreased).\n",
    "\n",
    "Now it's time to move it to an AWS instance in order to train.  I discovered my existing instance does not have any storage attached (I had been using a separate EBS volume, and deleted it after class).  Therefore, choosing a new instance:  <code>g2.2xlarge</code>, which has 1 GPU and 2 cores of CPU (2 threads each) and 16 GB of memory.\n",
    "\n",
    "System volume is /dev/sda1.  Working data storage is on /dev/sdb (60 GB).  Instance ID is i-0be255fe5cb1becf1.  Username is default \"ubuntu\".  Changed pwd to H1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
